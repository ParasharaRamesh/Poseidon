{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47024cb481badce7",
   "metadata": {},
   "source": [
    "# POSEIDON: Pose Estimation & Activity Recognition using GNNs\n",
    "\n",
    "Team Members (Group 16): \n",
    "1. Chong Jun Rong Brian (A0290882U)\n",
    "2. Parashara Ramesh (A0285647M)\n",
    "3. Ng Wei Jie Brandon (A0184893L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca470c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:11:37.226265Z",
     "start_time": "2024-11-17T07:11:37.179250Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775158eb81e1a9",
   "metadata": {},
   "source": [
    "NOTE: Need to add anchor jump links to each heading\n",
    "\n",
    "<h2><u> Table of contents </u></h2>\n",
    "\n",
    "1. What is this project about?\n",
    "<br> 1.1. Project Motivation\n",
    "<br> 1.2. Project Description\n",
    "<br> 1.3. Project Setup \n",
    "<br> 1.4. Project Changes from Initial Proposal\n",
    "<br> 1.5. Project Presentation Video\n",
    "\n",
    "2. Model building approach\n",
    "<br> 2.1. SimplePose (Baseline model) \n",
    "<br> 2.2. SimplePoseGNN\n",
    "<br> 2.3. SimplePoseGAT\n",
    "<br> 2.4. SimplePoseTAG\n",
    "\n",
    "3. Human 3.6M Dataset\n",
    "<br> 3.1. Summary of the dataset\n",
    "<br> 3.2. Preparing the dataset\n",
    "<br> 3.3. Visualizing poses\n",
    "<br> 3.4. Training baseline model (SimplePose)\n",
    "<br> 3.5. Training Graph Convolutional model (SimplePoseGNN)\n",
    "<br> 3.6. Training Graph Transformer model (SimplePoseGAT)\n",
    "<br> 3.7. Training Toplogy Adaptive Graph Convolutional model (SimplePoseTAG)\n",
    "\n",
    "4. Custom Dataset\n",
    "<br> 4.1. Summary of the dataset\n",
    "<br> 4.2. Preparing the dataset\n",
    "<br> 4.3. Visualizing poses\n",
    "<br> 4.4. Training baseline model (SimplePose)\n",
    "<br> 4.5. Training Graph Convolutional model (SimplePoseGNN)\n",
    "<br> 4.6. Training Graph Transformer model (SimplePoseGAT)\n",
    "<br> 4.7. Training Toplogy Adaptive Graph Convolutional model (SimplePoseTAG)\n",
    "\n",
    "5. Evaluation\n",
    "<br> 5.1. Human 3.6M dataset\n",
    "<br> 5.2. Custom Dataset\n",
    "\n",
    "6. Lessons Learnt & Conclusions\n",
    "7. Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d0baed6e093c8",
   "metadata": {},
   "source": [
    "<h2><u>1. What is this project about?</u></h2>\n",
    "<h3><u>1.1. Project Motivation</u></h3>\n",
    "\n",
    "Accurately predicting 3D human poses from 2D keypoints is essential for applications like motion capture and activity recognition. Traditional methods, such as direct regression or lifting techniques, often fail to capture the intricate spatial relationships between body joints. By modeling 2D pose keypoints as graphs, we can utilize the inherent connectivity between joints to enhance 3D pose estimation. Furthermore, recognizing and classifying human activities from these poses is vital in areas like surveillance and healthcare. This project aims to investigate how Graph Neural Networks (GNNs) can improve both 3D pose estimation and activity recognition.\n",
    "\n",
    "<h3><u>1.2. Project Description</h3></u>\n",
    "\n",
    "This project has two primary objectives.\n",
    "\n",
    "The first is to accurately predict 3D human poses from 2D keypoints using Graph Neural Networks (GNNs).\n",
    "\n",
    "* We will begin by developing two baseline models: one utilizing a standard Neural Network (NN) and Convolutional Neural Network (CNN), and the other employing a simple GNN for 3D pose estimation.\n",
    "* Next, we will reimplement the SemGCN model, which represents 2D pose keypoints as graph nodes with edges capturing the connectivity between joints.\n",
    "* Finally, we aim to enhance the SemGCN model by exploring alternative GNN architectures and introducing modifications to improve its performance.\n",
    "\n",
    "The second objective is to classify human activities based on 2D keypoints. For this task, we will use custom datasets to evaluate the generalization capabilities of GNN-based models for activity recognition.\n",
    "\n",
    "<h3><u>1.3. Project Setup</u></h3>\n",
    "\n",
    "(TODO need to rewrite this part)\n",
    "\n",
    "1. Install the dependencies from `requirements.txt` using the command\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "<h3><u>1.4. Project Changes from Initial Proposal</u></h3>\n",
    "\n",
    "TODO\n",
    "\n",
    "<h3><u>1.5. Project Presentation Video</u></h3>\n",
    "\n",
    "(TODO add our presentation slides as well)\n",
    "\n",
    "Our project presentation has been uploaded to youtube and can be found at <a href=\"https://youtu.be/O3Qb1UfjszM\">this link</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b83ba",
   "metadata": {},
   "source": [
    "<h2><u>2. Model Building Approach</u></h2>\n",
    "\n",
    "Previous works have attempted to do pose estimation and activity recognition via normal Neural Network methods such as Convolutional Neural Networks such as [PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes by Xiang. et al](https://arxiv.org/abs/1711.00199) and [Skeleton-based Human Action Recognition via\n",
    "Convolutional Neural Networks (CNN) by Ali. et al](https://arxiv.org/pdf/2301.13360).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f9218",
   "metadata": {},
   "source": [
    "<h3><u>2.1 SimplePose (Baseline model)</u></h3>\n",
    "\n",
    "To study the effectiveness of Graph Neural Networks in 3D Pose Estimation and Activity Recognition, we built our own basic Neural Network model which we will call it as SimplePose.\n",
    "\n",
    "SimplePose is a simple Neural Network model that uses the same model architecture as [A simple yet effective baseline for 3d human pose estimation by Martinez. et al](https://arxiv.org/abs/1705.03098) but with a few tweaks compared against the original architecture.\n",
    "\n",
    "These are the tweaks that we have done for SimplePose:-\n",
    "\n",
    "1. We increased the dropout rate from 0.5 to 0.6 to ensure stable training and to prevent overfitting.\n",
    "2. Instead of having 2 blocks as shown in Figure 1, we have 6 blocks because we found out it having bigger models enabled the multitask learning to perform well for SimplePose.\n",
    "3. After the last block, we add a small Neural Network for 3D Pose Estimation and another small Neural Network for Activity Recognition. Both of their inputs is the last block final output.\n",
    "\n",
    "You can see the SimplePose's implementation at [models/simple_pose.py](./models/simple_pose.py)\n",
    "\n",
    "The figure below illustrates the SimplePose Architecture:-\n",
    "\n",
    "<img title=\"SimplePose\" alt=\"SimplePose\" src=\"./notebook_images/SimplePose.drawio.png\">\n",
    "\n",
    "\n",
    "Below shows the number of trainable parameters for SimplePose:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simple_pose import SimplePose\n",
    "from utils.pytorch_utils import count_parameters\n",
    "model = SimplePose(total_joints=13, total_actions=5)\n",
    "parameters = count_parameters(model)\n",
    "print(f\"SimplePose Trainable Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd8e54",
   "metadata": {},
   "source": [
    "<h3><u>2.2 SimplePoseGNN</u></h3>\n",
    "\n",
    "We built 3 Graph Neural Networks to compare against SimplePose. Our first model is a Graph Convolutional Neural Network which we will call it as SimplePoseGNN.\n",
    "\n",
    "SimplePoseGNN is a Graph Convolutional Neural Network model that was proposed by [Kipf. et al](https://arxiv.org/pdf/1609.02907v4). We decided to implement this model because it is the baseline model for Graph Neural Networks.\n",
    "\n",
    "For our Graph Convolutional Layers for SimplePoseGNN, we used [GraphConv from DGL](https://docs.dgl.ai/en/1.1.x/generated/dgl.nn.pytorch.conv.GraphConv.html) in its default settings to build our own GraphConvModule.\n",
    "\n",
    "A GraphConvModule consists of these layers in sequence.:-\n",
    "1. GraphConv\n",
    "2. BatchNorm1D\n",
    "3. ReLU\n",
    "4. GraphConv\n",
    "5. BatchNorm1D\n",
    "6. Dropout \n",
    "7. ReLU\n",
    "8. Linear Layer\n",
    "   \n",
    "There is also a residual connection that connects from the input of the first GraphConv with the output from Linear Layer.\n",
    "\n",
    "SimplePoseGNN has 6 GraphConvModule to ensure good performance.\n",
    "\n",
    "Not only that, these are the additional modifications we have done for SimplePoseGNN:-\n",
    "\n",
    "1. We compute the Laplacian Positional Encoding of our graph inputs using [dgl.transforms.LapPE](https://docs.dgl.ai/en/1.1.x/generated/dgl.transforms.LapPE.html) with k = 5.\n",
    "2. We embed our Node Features, Laplacian Positional Encodings and Edge features into their own nn.Linear layers and concatenate the values before giving it to the first GraphConvModule.\n",
    "3. We add a small Neural Network for 3D Pose Estimation and Activity Recognition after the last GraphConvModule. Their inputs are the output of the last GraphConvModule.\n",
    "\n",
    "You can see the SimplePoseGNN's implementation at [models/simple_pose_gnn.py](./models/simple_pose_gnn.py)\n",
    "\n",
    "Here is SimplePoseGNN's architecture as described:-\n",
    "\n",
    "<img title=\"SimplePoseGNN\" alt=\"SimplePoseGNN\" src=\"./notebook_images/SimplePoseGNN.drawio.png\">\n",
    "\n",
    "Below shows the number of trainable parameters for SimplePoseGNN:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simple_pose_gnn import SimplePoseGNN\n",
    "from utils.pytorch_utils import count_parameters\n",
    "model = SimplePoseGNN(hidden_size=80, num_classes=5)\n",
    "parameters = count_parameters(model)\n",
    "print(f\"SimplePoseGNN Trainable Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d730a",
   "metadata": {},
   "source": [
    "<h3><u>2.3 SimplePoseGAT</u></h3>\n",
    "\n",
    "Our 2nd Graph Neural Network implementation is the Graph Transformer Neural Network which we denote it as SimplePoseGAT.\n",
    "\n",
    "SimplePoseGAT is a Graph Transformer Neural Network model that is proposed by [Dwivedi et al](https://arxiv.org/pdf/2012.09699v2). We decided to implement this model because we want to investigate if having attention scores can help improve the tasks' performance compared against all the other models.\n",
    "\n",
    "For this implementation, we followed the Graph Transformer's implementation as described in [DGL's Graph Transformers Tutorial](https://docs.dgl.ai/en/1.1.x/notebooks/sparse/graph_transformer.html) but with our own tweaks.\n",
    "\n",
    "These are the tweaks that we have done for SimplePoseGAT:-\n",
    "\n",
    "1. We compute the Laplacian Positional Encoding of our graph inputs using [dgl.transforms.LapPE](https://docs.dgl.ai/en/1.1.x/generated/dgl.transforms.LapPE.html) with k = 20.\n",
    "2. We embed our Node Features, Laplacian Positional Encodings and Edge features into their own nn.Linear layers and concatenate the values before giving it to the MultiHeadAttention Layer.\n",
    "3. We reduced the Graph Transformer Layers to 2 due to GPU memory limitations.\n",
    "4. We add a linear layer after the Graph Transformer to perform 3D Pose Estimations.\n",
    "5. We add a SumPooling layer followed by a small neural network model to perform Activity Recognition.\n",
    "\n",
    "You can see the SimplePoseGAT's implementation at [models/simple_pose_gat.py](./models/simple_pose_gat.py)\n",
    "\n",
    "The figure below illustrates the SimplePoseGAT Architecture:-\n",
    "\n",
    "<img title=\"SimplePoseGAT\" alt=\"SimplePoseGAT\" src=\"./notebook_images/SimplePoseGAT.drawio.png\">\n",
    "\n",
    "\n",
    "Below shows the number of trainable parameters for SimplePoseGAT:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simple_pose_gat import SimplePoseGAT\n",
    "from utils.pytorch_utils import count_parameters\n",
    "model = SimplePoseGAT(in_size=2, out_size=3, num_classes=5)\n",
    "parameters = count_parameters(model)\n",
    "print(f\"SimplePoseGAT Trainable Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e712f82",
   "metadata": {},
   "source": [
    "<h3><u>2.4 SimplePoseTAG</u></h3>\n",
    "\n",
    "For our final Graph Neural Network model, we built a Topology Adaptive Graph Convolutional Neural Network that was proposed by [Du. et al](https://arxiv.org/pdf/1710.10370).\n",
    "\n",
    "We will call this model as SimplePoseTAG. We decided to implement SimplePoseTAG because we want to see if allowing the model to interpret the topology of a graph can improve the tasks' performance as compared with other models\n",
    "\n",
    "For this implementation. we followed the same approach as SimplePoseGNN as stated in Section 2.2 with a few differences.\n",
    "\n",
    "1. We replaced the GraphConv layers with [TAGConv from DGL.](https://docs.dgl.ai/en/0.8.x/generated/dgl.nn.pytorch.conv.TAGConv.html). We also renamed this from GraphConvModule to TAGConvModule.\n",
    "\n",
    "You can see the SimplePoseTAG's implementation at [models/simple_pose_tag.py](./models/simple_pose_tag.py)\n",
    "\n",
    "The figure below illustrates the SimplePoseTAG Architecture:-\n",
    "\n",
    "<img title=\"SimplePoseTAG\" alt=\"SimplePoseTAG\" src=\"./notebook_images/SimplePoseTAG.drawio.png\">\n",
    "\n",
    "\n",
    "Below shows the number of trainable parameters for SimplePoseTAG:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74653c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simple_pose_tag import SimplePoseTAG\n",
    "from utils.pytorch_utils import count_parameters\n",
    "model = SimplePoseTAG(hidden_size=80, num_classes=5)\n",
    "parameters = count_parameters(model)\n",
    "print(f\"SimplePoseTAG Trainable Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611734fb42bb945e",
   "metadata": {},
   "source": [
    "<h2><u>3. Human 3.6M Dataset</u></h2>\n",
    "\n",
    "<h3><u>3.1. Summary of the dataset </u></h3>\n",
    "\n",
    "The [Human 3.6M dataset](http://vision.imar.ro/human3.6m/description.php) is a comprehensive dataset comprising 3.6 million 3D human poses, captured from 11 professional actors performing 17 everyday activities (e.g., talking, smoking, discussing).\n",
    "\n",
    "This dataset utilizes a 3D motion capture system with 10 cameras to track reflective markers placed on the body, enabling automatic labeling of joint positions.\n",
    "\n",
    "Participants were provided with detailed task instructions and visual examples of the actions to perform, but they were also allowed some freedom to move naturally during execution.\n",
    "\n",
    "As a result, the Human 3.6M dataset serves as a valuable resource for 3D human pose estimation, action recognition, and other computer vision research tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76972dcfdd4a2b57",
   "metadata": {},
   "source": [
    "<h3><u>3.2. Dataset preparation </u></h3>\n",
    "\n",
    "Downloading the Human 3.6M dataset directly from the official website requires a login. To simplify access, we will use a preprocessed version of the dataset, which is available on Google Drive [here](https://drive.google.com/file/d/1JGt3j9q5A8WzUY-QKfyMVJUUvfreri-s/view?usp=drive_link).\n",
    "\n",
    "The preprocessing was originally performed by [Martinez et al](https://github.com/una-dinosauria/3d-pose-baseline) and is sourced from their repository.\n",
    "\n",
    "The following cells outline the steps to download this preprocessed dataset and create train-test files after applying necessary transformations.\n",
    "\n",
    "From Section 3 (Models) onward, we will use the preprocessed train-test files directly. Therefore, executing the cells in this section is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b437e917fa318b",
   "metadata": {},
   "source": [
    "#### Downloading the zip from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1e0ceefe13857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:13:36.538742Z",
     "start_time": "2024-11-17T07:13:24.732200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the zip from google drive\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# file id from gdrive (refer to markdown cell above for full link)\n",
    "file_id = '1JGt3j9q5A8WzUY-QKfyMVJUUvfreri-s' \n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Folder where you want to save the file\n",
    "folder_path = os.path.join(os.getcwd(),\"datasets\", \"h36m\", \"Original\")\n",
    "zip_file_path = os.path.join(folder_path, 'h36m.zip')\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Download the ZIP file\n",
    "gdown.download(download_url, zip_file_path, quiet=False)\n",
    "\n",
    "print(f\"Downloaded ZIP file and saved to {zip_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af82d9932cf7c2",
   "metadata": {},
   "source": [
    "#### Extracting the contents into the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0822847be0d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:12:32.649677Z",
     "start_time": "2024-11-17T07:12:28.972734Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting the contents inside the /datasets/h36m/Original folder\n",
    "import zipfile\n",
    "\n",
    "print(f\"Extracting files to {folder_path}..\")\n",
    "# Open the ZIP file and extract its contents\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(folder_path)\n",
    "\n",
    "print(f\"Finished extracting files to {folder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5da612c5f96",
   "metadata": {},
   "source": [
    "#### Saving the 3d positions into a compressed np file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e513820c9281fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "output_filename = os.path.join(folder_path, 'data_3d_h36m')\n",
    "subjects = ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11']\n",
    "\n",
    "output = dict()\n",
    "for subject in tqdm(subjects, desc= f'Processing subjects..'):\n",
    "    output[subject] = dict()\n",
    "    file_list = glob(os.path.join(folder_path, 'h36m', subject, 'MyPoses', '3D_positions', '*.h5'))\n",
    "    assert len(file_list) == 30, \"Expected 30 files for subject \" + subject + \", got \" + str(len(file_list))\n",
    "    for f in file_list:\n",
    "        action = os.path.splitext(os.path.basename(f))[0]\n",
    "\n",
    "        if subject == 'S11' and action == 'Directions':\n",
    "            continue  # Discard corrupted video\n",
    "\n",
    "        with h5py.File(f) as hf:\n",
    "            positions = hf.get('3D_positions')[:].reshape(32, 3, -1).transpose(2, 0, 1)\n",
    "            positions /= 1000  # Meters instead of millimeters\n",
    "            output[subject][action] = positions.astype('float32')\n",
    "\n",
    "print(f'Saving compressed 3d positions into {output_filename}')\n",
    "np.savez_compressed(output_filename, positions_3d=output)\n",
    "print(f'Finished saving 3d positions into {output_filename}')\n",
    "del output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588965a522e8f8",
   "metadata": {},
   "source": [
    "#### Compute the ground truth 2d poses.\n",
    "\n",
    "The motion capture system records 3D poses using 10 cameras in the world coordinate frame. By applying the intrinsic and extrinsic matrices of each camera, these 3D poses are projected into 2D poses as they would appear in the respective camera's frame. The coordinates are then normalized so that the x, y, and z values range between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4794b93a1836eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.camera import world_to_camera, project_to_2d, image_coordinates, wrap\n",
    "from data.h36m_dataset import Human36mDataset\n",
    "\n",
    "print('Computing ground-truth 2D poses...')\n",
    "output_filename_2d = os.path.join(folder_path, 'data_2d_h36m_gt')\n",
    "\n",
    "dataset = Human36mDataset(output_filename + '.npz')\n",
    "output_2d_poses = {}\n",
    "for subject in dataset.subjects():\n",
    "    output_2d_poses[subject] = {}\n",
    "    for action in dataset[subject].keys():\n",
    "        anim = dataset[subject][action]\n",
    "\n",
    "        positions_2d = []\n",
    "        for cam in anim['cameras']:\n",
    "            pos_3d = world_to_camera(anim['positions'], R=cam['orientation'], t=cam['translation'])\n",
    "            pos_2d = wrap(project_to_2d, True, pos_3d, cam['intrinsic'])\n",
    "            pos_2d_pixel_space = image_coordinates(pos_2d, w=cam['res_w'], h=cam['res_h'])\n",
    "            positions_2d.append(pos_2d_pixel_space.astype('float32'))\n",
    "        output_2d_poses[subject][action] = positions_2d\n",
    "\n",
    "print(f'Saving compressed 2d positions into {output_filename_2d}')\n",
    "metadata = {\n",
    "    'num_joints': dataset.skeleton().num_joints(),\n",
    "    'keypoints_symmetry': [dataset.skeleton().joints_left(), dataset.skeleton().joints_right()]\n",
    "}\n",
    "np.savez_compressed(output_filename_2d, positions_2d=output_2d_poses, metadata=metadata)\n",
    "\n",
    "print(f'Done saving compressed 2d positions into {output_filename_2d}')\n",
    "del output_2d_poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72697b64fd7d407d",
   "metadata": {},
   "source": [
    "#### Cleanup the extracted files and the downloaded zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fd2a82b04cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "\n",
    "rmtree(os.path.join(folder_path, 'h36m'))\n",
    "os.remove(os.path.join(folder_path, 'h36m.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ac66ee540da7d",
   "metadata": {},
   "source": [
    "#### Creating and saving train and test files from the saved npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb37ba45841bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import read_3d_data, create_2d_data, create_train_test_files\n",
    "from data.h36m_dataset import TRAIN_SUBJECTS, TEST_SUBJECTS, Human36mDataset\n",
    "import os\n",
    "\n",
    "subjects_train = TRAIN_SUBJECTS\n",
    "subjects_test = TEST_SUBJECTS\n",
    "\n",
    "processed_dataset_path = os.path.join(os.getcwd(), \"datasets\", \"h36m\", \"Processed\")\n",
    "os.makedirs(processed_dataset_path, exist_ok=True)\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"datasets\", \"h36m\", \"Original\", \"data_3d_h36m.npz\")\n",
    "dataset = Human36mDataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35824cc0",
   "metadata": {},
   "source": [
    "#### Normalizing the 2D poses to [-1, 1] while preserving aspect ratio and 3D poses to camera's frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad8d6cc3eb740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading 3d npz file\")\n",
    "dataset = read_3d_data(dataset)\n",
    "\n",
    "print(\"Reading 2d npz file\")\n",
    "dataset_2d_path = os.path.join(os.getcwd(), \"datasets\", \"h36m\", \"Original\", \"data_2d_h36m_gt.npz\")\n",
    "keypoints = create_2d_data(dataset_2d_path, dataset)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58717c84",
   "metadata": {},
   "source": [
    "#### Select subset of distinct actions to train and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_actions = [\n",
    "    'Greeting',\n",
    "    'Photo',\n",
    "    'Sitting',\n",
    "    'SittingDown',\n",
    "    'Walking'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af75e0",
   "metadata": {},
   "source": [
    "#### Save the 2D poses, 3D poses, and actions in numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a656a402feba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating train datasets and saving into file\")\n",
    "_, _, _ = create_train_test_files(subjects_train, dataset, keypoints, \"train\", processed_dataset_path, keep_actions)\n",
    "\n",
    "print(\"Creating test datasets and saving into file\")\n",
    "_, _, _ = create_train_test_files(subjects_test, dataset, keypoints, \"test\", processed_dataset_path, keep_actions)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c43c8d75e4e921",
   "metadata": {},
   "source": [
    "<h3><u>3.3 Visualizing poses </u></h3>\n",
    "\n",
    "Since the train and test files have already been saved, we will use these files for the remainder of the notebook.\n",
    "\n",
    "These files are also available in this [Google Drive Link](https://drive.google.com/drive/folders/1d38hWSM8clZlI11nqljxBECog1KY-iTd?usp=sharing), which can be placed in the local path to skip the execution of earlier cells.\n",
    "\n",
    "The visualizations below indicate that, after the processing steps outlined above, human poses are represented as skeletons consisting of 13 joints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde57df9f7a0383",
   "metadata": {},
   "source": [
    "Let's visualise the poses for the 5 actions in the h3.6m dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc71b13b6e69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization_utils import visualize_2d_pose_actions\n",
    "\n",
    "out_poses_2d = np.load(\"datasets/h36m/Processed/test_2d_poses.npy\")\n",
    "out_actions = np.load(\"datasets/h36m/Processed/test_actions.npy\")\n",
    "for action in np.unique(out_actions):\n",
    "    out_poses_2d_action = out_poses_2d[out_actions == action]\n",
    "    visualize_2d_pose_actions(out_poses_2d_action, action=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82a59366a53d26",
   "metadata": {},
   "source": [
    "Let's visualise the first 2D pose and corresponding 3D poses in the h3.6m dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8ef64a0039560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.visualization_utils import visualize_2d_pose, visualize_3d_pose\n",
    "\n",
    "poses_2d = np.load(\"datasets/h36m/Processed/test_2d_poses.npy\")\n",
    "poses_3d = np.load(\"datasets/h36m/Processed/test_3d_poses.npy\")\n",
    "\n",
    "visualize_2d_pose(poses_2d[0])\n",
    "visualize_3d_pose(poses_3d[0], elev=110, azim=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7725e8082f63bef6",
   "metadata": {},
   "source": [
    "Let's visualise the distribution of the 5 actions in the h3.6m train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4ee6d0caadd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_action_stats(actions, type):\n",
    "    train_actions_df = pd.DataFrame(actions, columns=['actions'])\n",
    "    action_counts = train_actions_df['actions'].value_counts()\n",
    "    print(action_counts)\n",
    "    \n",
    "    action_counts.plot(kind='bar')\n",
    "    \n",
    "    # Step 4: Show the plot\n",
    "    plt.title(f'{type} Action Distribution')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d339da3ad4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions = np.load(\"datasets/h36m/Processed/train_actions.npy\")\n",
    "show_action_stats(train_actions, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e77e8dbb1d9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actions = np.load(\"datasets/h36m/Processed/test_actions.npy\")\n",
    "show_action_stats(test_actions, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.h36m_graph_loader_with_edge_feats import Human36MGraphEdgeDataset\n",
    "import networkx as nx\n",
    "import dgl\n",
    "\n",
    "item = {\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "}\n",
    "\n",
    "training_data = Human36MGraphEdgeDataset(item['training_2d_data_path'], item['training_3d_data_path'], item['training_label_path'])\n",
    "g = training_data[0][0]\n",
    "print(g)\n",
    "options = {\n",
    "    'node_size': 100,\n",
    "    'width': 1,\n",
    "}\n",
    "G = dgl.to_networkx(g)\n",
    "plt.figure(figsize=[15,7])\n",
    "nx.draw(G, **options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016dbee3",
   "metadata": {},
   "source": [
    "**NOTE: do not run the cells below as the output contains our training execution logs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5d78a",
   "metadata": {},
   "source": [
    "<h3><u>3.4.  Training baseline model (SimplePose) </u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0032857",
   "metadata": {},
   "source": [
    "<h3><u>3.5.  Training Graph Convolutional model (SimplePoseGNN) </u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose_gnn.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gnn', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae590ec",
   "metadata": {},
   "source": [
    "<h3><u>3.6.  Training Graph Transformer model (SimplePoseGAT) </u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose_gat.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gat', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1ece2",
   "metadata": {},
   "source": [
    "<h3><u>3.7.  Training Toplogy Adaptive Graph Convolutional model (SimplePoseTAG) </u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose_tag.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gat', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726f8b0",
   "metadata": {},
   "source": [
    "<h2><u>4. Custom Dataset</u></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d743e",
   "metadata": {},
   "source": [
    "<h3><u>4.1. Summary of the dataset </u></h3>\n",
    "\n",
    "Alongside the Human 3.6M dataset, we created a custom dataset containing 34,594 human poses. These poses were recorded from a single individual performing five distinct actions, captured from ten different camera angles.\n",
    "\n",
    "Instead of utilizing a 3D motion capture system, we recorded videos using a digital camera equipped with a global shutter. The 2D and 3D poses were labeled using MediaPipe.\n",
    "\n",
    "To introduce natural variability, we allowed the individual to incorporate turns and movements while performing the actions, ensuring the dataset reflects more realistic motion patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4b009",
   "metadata": {},
   "source": [
    "<h3><u>4.2 Preparing the dataset</u></h3>\n",
    "\n",
    "We selected five distinct actions for capture: \"arm_stretch,\" \"leg_stretch,\" \"lunges,\" \"side_stretch,\" and \"walking.\" These actions were recorded using a camera mounted on a stand with a global shutter to eliminate motion blur. The recordings were taken from various camera positions and angles to introduce variability.\n",
    "\n",
    "Here are the steps to record and prepare your custom dataset:\n",
    "\n",
    "1. Use the Python script `data_collection/record_video.py` to record the actions and save the videos in MP4 format.\n",
    "2. Use the Python script `data_collection/extract_poses.py` to extract 2D and 3D poses with MediaPipe, which automatically labels the poses, and save the poses in Numpy format. Ensure the action names and video index filenames are updated correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087664a",
   "metadata": {},
   "source": [
    "We included a sample video located at `data_collection/sample.mp4`. Running the script `data_collection/check_video.py` will display the following clip: the left side shows the original video, while the right side displays the annotated version with poses marked.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img alt=\"Sample Mediapose Output\" width=\"100%\" src=\"data_collection/sample.gif\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85feccb",
   "metadata": {},
   "source": [
    "<h3><u>4.3 Visualizing poses </u></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d767640",
   "metadata": {},
   "source": [
    "Let's visualise the poses for the 5 actions in the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization_utils import visualize_2d_pose_actions_custom\n",
    "\n",
    "out_poses_2d = np.load(\"datasets/custom/Processed/test_2d_poses.npy\")\n",
    "out_actions = np.load(\"datasets/custom/Processed/test_actions.npy\")\n",
    "\n",
    "for action in np.unique(out_actions):\n",
    "    out_poses_2d_action = out_poses_2d[out_actions == action]\n",
    "    visualize_2d_pose_actions_custom(out_poses_2d_action, action=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d4d7d",
   "metadata": {},
   "source": [
    "Let's visualise the first 2D pose and corresponding 3D poses in the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee629e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.visualization_utils import visualize_2d_pose_custom, visualize_3d_pose_custom\n",
    "\n",
    "poses_2d = np.load(\"datasets/custom/Processed/test_2d_poses.npy\")\n",
    "poses_3d = np.load(\"datasets/custom/Processed/test_3d_poses.npy\")\n",
    "\n",
    "visualize_2d_pose_custom(poses_2d[0])\n",
    "visualize_3d_pose_custom(poses_3d[0], elev=110, azim=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f66be5",
   "metadata": {},
   "source": [
    "Let's visualise the distribution of the 5 actions in the custom train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0262d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions = np.load(\"datasets/custom/Processed/train_actions.npy\")\n",
    "show_action_stats(train_actions, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actions = np.load(\"datasets/custom/Processed/test_actions.npy\")\n",
    "show_action_stats(test_actions, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac64500",
   "metadata": {},
   "source": [
    "<h3><u>4.4. Training baseline model (SimplePose)</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d06847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'custom', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'custom', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f8efcd",
   "metadata": {},
   "source": [
    "<h3><u>4.5. Training Graph Convolutional model (SimplePoseGNN)</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose_gnn.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'custom', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'custom', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gnn', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ffa2b",
   "metadata": {},
   "source": [
    "<h3><u>4.6. Training Graph Transformer model (SimplePoseGAT)</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose_gat.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'custom', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'custom', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gat', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c042a",
   "metadata": {},
   "source": [
    "<h3><u>4.7. Training Toplogy Adaptive Graph Convolutional model (SimplePoseTAG)</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd02582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simple_pose_tag.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 1,\n",
    "    'pose_loss_multiplier': 100,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'custom', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'custom', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'custom', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gat', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f66435",
   "metadata": {},
   "source": [
    "<h2><u>5. Evaluation </u></h2>\n",
    "\n",
    "In this section we will be showing the training and testing graphs for each model across both datasets along with confusion matrices for each model. \n",
    "\n",
    "Before you run the cells below, you will need to download the final_weights.zip from our Google Drive Link.\n",
    "\n",
    "**Please run all the cells in this section to generate the graphs and the confusion matrices. We use the same image names for our explanation below**\n",
    "\n",
    "To evaluate our models, we have decided to use these metrics.\n",
    "\n",
    "1. Pose Loss: To measure the pose loss per epoch using Mean Squared Loss\n",
    "2. Label Loss: To measure the label loss per epoch using Cross Entropy Loss\n",
    "3. Total Loss: To measure our multitasking loss per epoch using this formula: pose_loss_multiplier * pose_loss + action_loss_multiplier * action_loss where pose_loss_multiplier = 100 and action_loss_multiplier = 1\n",
    "4. Accuracies: To measure the number of right label predictions per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e8b61",
   "metadata": {},
   "source": [
    "<h3><u>5.1 Human 3.6M Dataset </u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8490601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:01<00:00, 236.05it/s] 243.45it/s]\n",
      "Testing =>: 100%|██████████| 293/293 [00:01<00:00, 235.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 293/293 [01:35<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 293/293 [01:48<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 293/293 [01:35<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the loss and accuracies graphs\n",
    "import os\n",
    "from utils.visualization_utils import save_fig\n",
    "\n",
    "TESTING_2D_DATA_PATH = os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy')\n",
    "TESTING_3D_DATA_PATH =  os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy')\n",
    "TESTING_LABEL_PATH = os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy')\n",
    "SAVE_PATH = os.path.join('visualizations', 'graphs', 'h36m')\n",
    "\n",
    "save_files = {\n",
    "    'SimplePose': os.path.join('final_weights', 'h36m', 'simple_pose_h36m.pth'),\n",
    "    'SimplePoseGNN': os.path.join('final_weights', 'h36m', \"simple_pose_gnn_h36m.pth\"),\n",
    "    'SimplePoseGAT': os.path.join('final_weights', 'h36m', \"simple_pose_gat_h36m.pth\"), \n",
    "    'SimplePoseTAG': os.path.join('final_weights', 'h36m', \"simple_pose_tag_h36m.pth\"),\n",
    "}\n",
    "\n",
    "save_fig(save_files, 'h36m', SAVE_PATH)\n",
    "\n",
    "\n",
    "from src.simple_pose.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePose'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")\n",
    "\n",
    "from src.simple_pose_gnn.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePoseGNN'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")\n",
    "\n",
    "from src.simple_pose_gat.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePoseGAT'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")\n",
    "\n",
    "from src.simple_pose_tag.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePoseTAG'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a6ad6",
   "metadata": {},
   "source": [
    "<h4>Accuracies and Confusion Matrices</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25236790",
   "metadata": {},
   "source": [
    "<h5>Human 3.6M Confusion Matrix</h5>\n",
    "\n",
    "\n",
    "| SimplePose| SimplePoseGNN |\n",
    "|---------------|---------------|\n",
    "|![SimplePose](./visualizations/graphs/h36m/SimplePose_conf_matrix.png)|![SimplePoseGNN](./visualizations/graphs/h36m/SimplePoseGNN_conf_matrix.png)|\n",
    "|SimplePoseGAT| SimplePoseTAG|\n",
    "| ![SimplePoseGAT](./visualizations/graphs/h36m/SimplePoseGAT_conf_matrix.png) | ![SimplePoseTAG](./visualizations/graphs/h36m/SimplePoseTAG_conf_matrix.png) |\n",
    "\n",
    "\n",
    "<h5>Human 3.6M Accuracies</h5>\n",
    "\n",
    "| All Models Training Accuracies | All Models Testing Accuracies |\n",
    "|---------------|---------------|\n",
    "|![All Models Training Accuracies ](./visualizations/graphs/h36m/h36m_training_accuracies.png)|![All Models Testing Accuracies](./visualizations/graphs/h36m/h36m_testing_accuracies.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd63933",
   "metadata": {},
   "source": [
    "<h3><u>5.2 Custom Dataset </u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bb5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8e4486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 83.70it/s] 84.20it/s]\n",
      "Testing =>: 100%|██████████| 10/10 [00:00<00:00, 82.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 10/10 [00:03<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 10/10 [00:06<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Without Normalization\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the loss and accuracies graphs\n",
    "import os\n",
    "from utils.visualization_utils import save_fig\n",
    "\n",
    "TESTING_2D_DATA_PATH = os.path.join('datasets', 'custom', 'Processed', 'test_2d_poses.npy')\n",
    "TESTING_3D_DATA_PATH =  os.path.join('datasets', 'custom', 'Processed', 'test_3d_poses.npy')\n",
    "TESTING_LABEL_PATH = os.path.join('datasets', 'custom', 'Processed', 'test_actions.npy')\n",
    "SAVE_PATH = os.path.join('visualizations', 'graphs', 'custom')\n",
    "\n",
    "save_files = {\n",
    "    'SimplePose': os.path.join('final_weights', 'custom', 'simple_pose_custom.pth'),\n",
    "    'SimplePoseGNN': os.path.join('final_weights', 'custom', \"simple_pose_gnn_custom.pth\"),\n",
    "    'SimplePoseGAT': os.path.join('final_weights', 'custom', \"simple_pose_gat_custom.pth\"), \n",
    "    'SimplePoseTAG': os.path.join('final_weights', 'custom', \"simple_pose_tag_custom.pth\"),\n",
    "}\n",
    "\n",
    "save_fig(save_files, 'custom', SAVE_PATH)\n",
    "\n",
    "\n",
    "from src.simple_pose.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePose'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")\n",
    "\n",
    "from src.simple_pose_gnn.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePoseGNN'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")\n",
    "\n",
    "from src.simple_pose_gat.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePoseGAT'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")\n",
    "\n",
    "from src.simple_pose_tag.train_and_test import generate_confusion_matrix\n",
    "generate_confusion_matrix(\n",
    "    weights_path=save_files['SimplePoseTAG'],\n",
    "    save_path=SAVE_PATH,\n",
    "    testing_2d_path= TESTING_2D_DATA_PATH,\n",
    "    testing_3d_path= TESTING_3D_DATA_PATH,\n",
    "    testing_label_path= TESTING_LABEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a2fd0",
   "metadata": {},
   "source": [
    "<h2><u>6. Lessons Learnt & Conclusions </u></h2>\n",
    "\n",
    "\n",
    "Talk about the history of development and optimizations we made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de2aba",
   "metadata": {},
   "source": [
    "<h2><u>7. Resources</u></h2>\n",
    "\n",
    "\n",
    "TODO add blogs, documentations , us"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
