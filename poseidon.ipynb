{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47024cb481badce7",
   "metadata": {},
   "source": [
    "# POSEIDON: Pose Estimation & Activity Recognition using GNNs\n",
    "\n",
    "Team Members (Group 16): \n",
    "1. Chong Jun Rong Brian (A0290882U)\n",
    "2. Parashara Ramesh (A0285647M)\n",
    "3. Ng Wei Jie Brandon (A0184893L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775158eb81e1a9",
   "metadata": {},
   "source": [
    "<h2><u> Table of contents </u></h2>\n",
    "\n",
    "1. What is this project about?\n",
    "<br> 1.1. Project Motivation\n",
    "<br> 1.2. Project Description\n",
    "<br> 1.3. Project Setup   \n",
    "2. Human 3.6M Dataset\n",
    "<br> 2.1. Summary of the dataset\n",
    "<br> 2.2. Preparing the dataset\n",
    "<br> 2.3. EDA\n",
    "3. Dataset preparation\n",
    "4. Models\n",
    "5. Baseline 1 - SimplePose (Simple ML model without using GNNs)\n",
    "6. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs) \n",
    "7. Improvement 1 - SemGCN model (Reimplementation of Semantic GCN)\n",
    "8. Improvement 2 - PoseGCN model (Tweaks of SemGCN)\n",
    "9. Evaluation & Analysis of models\n",
    "10. Creating our own custom dataset\n",
    "11. Evaluation on custom dataset\n",
    "12. Conclusion\n",
    "13. Video presentation & Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d0baed6e093c8",
   "metadata": {},
   "source": [
    "<h2><u>1. What is this project about?</u></h2>\n",
    "<h3><u>1.1 Project Motivation</u></h3>\n",
    "\n",
    "Accurately predicting 3D human poses from 2D keypoints is a critical task for many applications such as motion capture and activity recognition. Traditional methods that use direct regression or lifting techniques often struggle to fully capture the complex spatial relationships between body joints. By treating the 2D pose keypoints as graphs, we can leverage the underlying connectivity between joints to improve the 3D pose estimation. Additionally, recognizing and classifying human activities from these poses is an essential task in fields like surveillance and healthcare. Therefore, this project seeks to explore how GNNs can enhance 3D pose estimation and activity recognition.\n",
    "\n",
    "<h3><u>1.2 Project Description</h3></u>\n",
    "\n",
    "The primary objective of this project is to predict 3D human poses from 2D pose keypoints accurately using GNNs. \n",
    "* Firstly, we will develop two baseline models: one using standard Neural Network (NN) & Convolutional Neural Network (CNN) followed by a simple GNN based model both for 3d pose estimation \n",
    "* Secondly, we will reimplement the SemGCN model, which treats the body joints of a 2D pose as nodes in a graph, with edges representing the connectivity between them. \n",
    "* Finally, we will design an improved version of the SemGCN model by exploring different GNN architectures and modifications to enhance its performance.\n",
    "\n",
    "The secondary objective is to classify human activities based on 2D pose keypoints. We will use custom datasets to validate this task, allowing us to assess the generalization capabilities of GNN-based models for activity recognition.\n",
    "\n",
    "<h3><u>1.3 Project Setup</u></h3>\n",
    "\n",
    "1. Install the dependencies from requirements.txt (TODO.all to fix later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f092b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fd3e6fe88e214",
   "metadata": {},
   "source": [
    "<h2><u>2. Human 3.6M Dataset</u></h2>\n",
    "\n",
    "<h3><u>2.1 Summary of the dataset </u></h3>\n",
    "\n",
    "The [Human 3.6M dataset](http://vision.imar.ro/human3.6m/description.php) is a large-scale collection of 3.6 million 3D human poses captured from 11 professional actors in 17 everyday scenarios (e.g., talking, smoking, discussing). It includes synchronized video from multiple calibrated cameras, precise 3D joint positions and angles, and pixel-level body part labels. Additional data like time-of-flight range data and 3D laser scans of actors are also available.\n",
    "\n",
    "The dataset comes with precomputed image descriptors, tools for visualizing and predicting human poses, and an evaluation set for benchmarking performance, making it a rich resource for 3D human pose estimation, action recognition, and related computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76972dcfdd4a2b57",
   "metadata": {},
   "source": [
    "<h3><u>2.2. Dataset preparation </u></h3>\n",
    "\n",
    "Downloading the dataset from the website requires a login , therefore we will be directly using a preprocessed version of this dataset stored in google drive [here](https://drive.google.com/file/d/1JGt3j9q5A8WzUY-QKfyMVJUUvfreri-s/view?usp=drive_link).\n",
    " \n",
    "Original preprocessing was done by [Martinez et al](https://github.com/una-dinosauria/3d-pose-baseline) from this repository.\n",
    "\n",
    "The cells below go through the steps of downloading this preprocessed dataset and creating train-test files from it after some transformation. \n",
    "\n",
    "From section 3(Models) we will be directly using the created train-test files, therefore the cells under this section need not be executed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71f1e0ceefe13857"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3><u>2.3. EDA </u></h3>\n",
    "\n",
    "TODO.brandon :\n",
    "- add h3 tags for each subsection + make changes in table of contents + only present the story points here, main code can go to appropriate folders\n",
    "\n",
    "EDA points will also come here as individual cell blocks but will be called as one function directly in the dataset preparation"
   ],
   "id": "5eda21fa89e067e6"
  },
  {
   "cell_type": "markdown",
   "id": "ad19bb814deadd8b",
   "metadata": {},
   "source": [
    "<h2><u>3. Models</u></h2>\n",
    "\n",
    "TODO.all - write a convincing story on our approach + high level thoughts on why the following models are worth building and what we hope to gain from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0444a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Model based on A Simple yet effective baseline for 3D Pose Estimation\n",
    "class LinearBaselineModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, total_joints, total_actions):\n",
    "        super().__init__()\n",
    "        self.total_joints = total_joints,\n",
    "        self.total_actions = total_actions\n",
    "        self.input_linear = nn.Linear(total_joints * 2, 1024) # 1d input shape is B x 16 x 2\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.output_3d_pose_linear = nn.Linear(1024, total_joints * 3) # 3D output shape is B x 16 x 3\n",
    "        self.output_label_linear = nn.Linear(1024, total_actions) # Predict Action Labels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.input_linear(x)\n",
    "        x = self.block1(x) + x # First Residual connection\n",
    "        x = self.block2(x) + x # Second Residual Connection\n",
    "        three_dim_pose_predictions = self.output_3d_pose_linear(x)\n",
    "        action_label_predictions = self.output_label_linear(x)\n",
    "        joint_preds = three_dim_pose_predictions.view(x.shape[0], -1, 3)\n",
    "        action_preds = action_label_predictions\n",
    "        return joint_preds, action_preds   # 3D output shape is B x 16 x 3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f319c5978c85c1",
   "metadata": {},
   "source": [
    "<h2><u>4. Baseline 1 - SimplePose (Simple ML model without using GNNs)</u></h2>\n",
    "\n",
    "TODO.brian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "091fa9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class Human36MDataset(Dataset):\n",
    "    def __init__(self, two_d_dataset_path, three_d_dataset_path, label_dataset_path):\n",
    "        self.two_d_dataset_path = two_d_dataset_path\n",
    "        self.three_d_dataset_path = three_d_dataset_path\n",
    "        self.label_dataset_path = label_dataset_path\n",
    "        self.input_data = np.load(self.two_d_dataset_path)\n",
    "        self.output_data = np.load(self.three_d_dataset_path)\n",
    "        self.labels = np.load(self.label_dataset_path)\n",
    "        unique_labels, tags = np.unique(self.labels, return_inverse=True)\n",
    "        self.unique_labels = unique_labels\n",
    "        self.labels = tags\n",
    "        self.labels_map = dict(zip(range(len(unique_labels)),unique_labels))\n",
    "        assert len(self.input_data) == len(self.labels) == len(self.output_data)\n",
    "    \n",
    "    def get_action_numbers(self):\n",
    "        return len(self.unique_labels)\n",
    "    \n",
    "    def get_joint_numbers(self):\n",
    "        return self.input_data[0].shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_data[index], self.output_data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f272c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 4304959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24372/24372 [01:22<00:00, 294.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Total Training Loss: 5.275071620941162 | Pose Training Loss: 2.327385663986206 | Action Training Loss: 2.947679042816162 | Action Train Label Accuracy: 20.014463837840886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8490/8490 [00:55<00:00, 151.78it/s]\n",
      "  0%|          | 1/200 [02:20<7:45:55, 140.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Total Testing Loss: 1.9314818382263184 | Pose Testing Loss: 0.8192713260650635 | Action Testing Loss: 1.1122114658355713 | Action Test Label Accuracy: 16.10195382667334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3227/24372 [01:28<09:40, 36.45it/s]\n",
      "  0%|          | 1/200 [03:49<12:39:46, 229.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 55\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dataloader):\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m# Prepare Data\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     two_dim_input_data, three_dim_output_data, action_labels\u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m---> 55\u001B[0m     two_dim_input_data \u001B[38;5;241m=\u001B[39m \u001B[43mtwo_dim_input_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m     three_dim_output_data \u001B[38;5;241m=\u001B[39m three_dim_output_data\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[0;32m     57\u001B[0m     action_labels \u001B[38;5;241m=\u001B[39m action_labels\u001B[38;5;241m.\u001B[39mto(DEVICE)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu') \n",
    "\n",
    "training_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy')\n",
    "training_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy')\n",
    "training_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy')\n",
    "training_data = Human36MDataset(training_2d_dataset_path, training_3d_dataset_path, training_label_path)\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testing_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy')\n",
    "testing_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy')\n",
    "testing_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy')\n",
    "testing_data = Human36MDataset(testing_2d_dataset_path, testing_3d_dataset_path, testing_label_path)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "def kaiming_weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "TOTAL_JOINTS = training_data.get_joint_numbers()\n",
    "TOTAL_ACTIONS = training_data.get_action_numbers()\n",
    "\n",
    "# Declare Model\n",
    "model = LinearBaselineModel(TOTAL_JOINTS, TOTAL_ACTIONS).to(DEVICE)\n",
    "# Apply Kaiming Init on Linear Layers\n",
    "model.apply(kaiming_weights_init)\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Declare Optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Declare Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96) # Value used by the original authors\n",
    "\n",
    "# Loss Function\n",
    "three_dim_pose_estimation_loss_fn = nn.MSELoss()\n",
    "action_label_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "global_total_losses = []\n",
    "global_pose_losses = []\n",
    "global_action_losses = []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    predicted_labels = None\n",
    "    true_labels = None\n",
    "    total_losses = []\n",
    "    pose_losses = []\n",
    "    action_losses = []\n",
    "    for data in tqdm(train_dataloader):\n",
    "        # Prepare Data\n",
    "        two_dim_input_data, three_dim_output_data, action_labels= data\n",
    "        two_dim_input_data = two_dim_input_data.to(DEVICE)\n",
    "        three_dim_output_data = three_dim_output_data.to(DEVICE)\n",
    "        action_labels = action_labels.to(DEVICE)\n",
    "        # Set Gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        # Train Model\n",
    "        predicted_3d_pose_estimations, predicted_action_labels = model(two_dim_input_data)\n",
    "        # Calculate Loss\n",
    "        three_dim_pose_estimation_loss = three_dim_pose_estimation_loss_fn(predicted_3d_pose_estimations, three_dim_output_data)\n",
    "        action_label_loss = action_label_loss_fn(predicted_action_labels, action_labels)\n",
    "        loss = three_dim_pose_estimation_loss + action_label_loss\n",
    "        if epoch % 10 == 0: # Every 10 epochs, report once\n",
    "            # Store Results\n",
    "            total_losses.append(loss)\n",
    "            pose_losses.append(three_dim_pose_estimation_loss)\n",
    "            action_losses.append(action_label_loss)\n",
    "            predicted_action_labels = torch.argmax(predicted_action_labels, axis=1)\n",
    "            predicted_labels = predicted_action_labels if predicted_labels is None else torch.cat((predicted_labels, predicted_action_labels), axis=0)\n",
    "            true_labels = action_labels if true_labels is None else torch.cat((true_labels, action_labels), axis=0)\n",
    "        # Optimize Gradients and Update Learning Rate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Every 10 epochs, report once\n",
    "    if epoch % 10 == 0:\n",
    "        correct_predictions = (predicted_labels == true_labels).sum().item()\n",
    "        accuracy = correct_predictions / predicted_labels.size(0) * 100\n",
    "        total_loss = sum(total_losses) / len(train_dataloader)\n",
    "        pose_loss = sum(pose_losses) / len(train_dataloader)\n",
    "        action_loss = sum(action_losses) / len(train_dataloader)\n",
    "        global_total_losses.append(total_loss)\n",
    "        global_action_losses.append(action_loss)\n",
    "        global_pose_losses.append(pose_loss)\n",
    "        print(f\"Epoch: {epoch} | Total Training Loss: {total_loss} | Pose Training Loss: {pose_loss} | Action Training Loss: {action_loss} | Action Train Label Accuracy: {accuracy}\")\n",
    "    \n",
    "    if epoch % 10 == 0: # Every 10 epochs, test once\n",
    "        predicted_labels = None\n",
    "        true_labels = None\n",
    "        total_losses = []\n",
    "        pose_losses = []\n",
    "        action_losses = []\n",
    "        for data in tqdm(test_dataloader):\n",
    "            # Prepare Data\n",
    "            two_dim_input_data, three_dim_output_data, action_labels= data\n",
    "            two_dim_input_data = two_dim_input_data.to(DEVICE)\n",
    "            three_dim_output_data = three_dim_output_data.to(DEVICE)\n",
    "            action_labels = action_labels.to(DEVICE)\n",
    "            # Predict with model\n",
    "            predicted_3d_pose_estimations, predicted_action_labels = model(two_dim_input_data)\n",
    "            # Calculate Loss\n",
    "            three_dim_pose_estimation_loss = three_dim_pose_estimation_loss_fn(predicted_3d_pose_estimations, three_dim_output_data)\n",
    "            action_label_loss = action_label_loss_fn(predicted_action_labels, action_labels)\n",
    "            loss = three_dim_pose_estimation_loss + action_label_loss\n",
    "            # Store Results\n",
    "            total_losses.append(loss)\n",
    "            pose_losses.append(three_dim_pose_estimation_loss)\n",
    "            action_losses.append(action_label_loss)\n",
    "            predicted_action_labels = torch.argmax(predicted_action_labels, axis=1)\n",
    "            predicted_labels = predicted_action_labels if predicted_labels is None else torch.cat((predicted_labels, predicted_action_labels), axis=0)\n",
    "            true_labels = action_labels if true_labels is None else torch.cat((true_labels, action_labels), axis=0)\n",
    "        \n",
    "        # Calculate Test Accuracy\n",
    "        correct_predictions = (predicted_labels == true_labels).sum().item()\n",
    "        accuracy = correct_predictions / predicted_labels.size(0) * 100\n",
    "        total_loss = sum(total_losses) / len(train_dataloader)\n",
    "        pose_loss = sum(pose_losses) / len(train_dataloader)\n",
    "        action_loss = sum(action_losses) / len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch} | Total Testing Loss: {total_loss} | Pose Testing Loss: {pose_loss} | Action Testing Loss: {action_loss} | Action Test Label Accuracy: {accuracy}\")\n",
    "    \n",
    "# Save model\n",
    "state_dict = {\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model': model.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "}\n",
    "weight_save_path = os.path.join('weights', 'linear_baseline_model')\n",
    "if not os.path.exists(weight_save_path):\n",
    "    os.makedirs(weight_save_path)\n",
    "\n",
    "torch.save(state_dict, os.path.join(weight_save_path, 'weights.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8d37443b92adc",
   "metadata": {},
   "source": [
    "<h2><u>5. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs)</u></h2>\n",
    "\n",
    "TODO.brandon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386900b282f7968",
   "metadata": {},
   "source": [
    "<h2><u>6. Improvement 1 - SemGCN model (Reimplementation of Semantic GCN)</u></h2>\n",
    "\n",
    "TODO.parash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eddcac2a537037",
   "metadata": {},
   "source": [
    "<h2><u>7. Improvement 2 - PoseGCN model (Tweaks of SemGCN)</u></h2>\n",
    "\n",
    "TODO.all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbad889b6bc016",
   "metadata": {},
   "source": "<h2><u>8. Evaluation & Analysis of models<u></h2>"
  },
  {
   "cell_type": "markdown",
   "id": "49da5ad156e80a02",
   "metadata": {},
   "source": "<h2><u>9. Creating our own custom dataset</u></h2>"
  },
  {
   "cell_type": "markdown",
   "id": "477bbc24b9d697ad",
   "metadata": {},
   "source": "<h2><u>10. Evaluation on custom dataset</u></h2>"
  },
  {
   "cell_type": "markdown",
   "id": "27e11b1939fb52fe",
   "metadata": {},
   "source": "<h2><u>11. Conclusion</u></h2>"
  },
  {
   "cell_type": "markdown",
   "id": "cdca78e7d8d35b85",
   "metadata": {},
   "source": "<h2><u>12. Video presentation & Resources</u></h2>"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
