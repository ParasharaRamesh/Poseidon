{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47024cb481badce7",
   "metadata": {},
   "source": [
    "# POSEIDON: Pose Estimation & Activity Recognition using GNNs\n",
    "\n",
    "Team Members (Group 16): \n",
    "1. Chong Jun Rong Brian (A0290882U)\n",
    "2. Parashara Ramesh (A0285647M)\n",
    "3. Ng Wei Jie Brandon (A0184893L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca470c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T07:02:29.934709Z",
     "start_time": "2024-11-02T07:02:29.892153Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775158eb81e1a9",
   "metadata": {},
   "source": [
    "<h2><u> Table of contents </u></h2>\n",
    "\n",
    "1. What is this project about?\n",
    "<br> 1.1. Project Motivation\n",
    "<br> 1.2. Project Description\n",
    "<br> 1.3. Project Setup   \n",
    "2. Human 3.6M Dataset\n",
    "<br> 2.1. Summary of the dataset\n",
    "<br> 2.2. Preparing the dataset\n",
    "<br> 2.3. Visualizing poses\n",
    "3. Model building approach\n",
    "4. Baseline 1 - SimplePose (Simple ML model without using GNNs)\n",
    "5. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs) \n",
    "6. Improvement 1 - SimplePoseGAT (Using 2 layers of GATConv)\n",
    "7. Improvement 2 - SemGCN model (Reimplementation of Semantic GCN)\n",
    "8. Evaluation & Analysis of models\n",
    "9. Creating our own custom dataset\n",
    "10. Evaluation on custom dataset\n",
    "11. Conclusion\n",
    "12. Video presentation & Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d0baed6e093c8",
   "metadata": {},
   "source": [
    "<h2><u>1. What is this project about?</u></h2>\n",
    "<h3><u>1.1 Project Motivation</u></h3>\n",
    "\n",
    "Accurately predicting 3D human poses from 2D keypoints is a critical task for many applications such as motion capture and activity recognition. Traditional methods that use direct regression or lifting techniques often struggle to fully capture the complex spatial relationships between body joints. By treating the 2D pose keypoints as graphs, we can leverage the underlying connectivity between joints to improve the 3D pose estimation. Additionally, recognizing and classifying human activities from these poses is an essential task in fields like surveillance and healthcare. Therefore, this project seeks to explore how GNNs can enhance 3D pose estimation and activity recognition.\n",
    "\n",
    "<h3><u>1.2 Project Description</h3></u>\n",
    "\n",
    "The primary objective of this project is to predict 3D human poses from 2D pose keypoints accurately using GNNs. \n",
    "* Firstly, we will develop two baseline models: one using standard Neural Network (NN) & Convolutional Neural Network (CNN) followed by a simple GNN based model both for 3d pose estimation \n",
    "* Secondly, we will reimplement the SemGCN model, which treats the body joints of a 2D pose as nodes in a graph, with edges representing the connectivity between them. \n",
    "* Finally, we will design an improved version of the SemGCN model by exploring different GNN architectures and modifications to enhance its performance.\n",
    "\n",
    "The secondary objective is to classify human activities based on 2D pose keypoints. We will use custom datasets to validate this task, allowing us to assess the generalization capabilities of GNN-based models for activity recognition.\n",
    "\n",
    "<h3><u>1.3 Project Setup</u></h3>\n",
    "\n",
    "1. Install the dependencies from requirements.txt using the command\n",
    "\n",
    "`pip install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611734fb42bb945e",
   "metadata": {},
   "source": [
    "<h2><u>2. Human 3.6M Dataset</u></h2>\n",
    "\n",
    "<h3><u>2.1 Summary of the dataset </u></h3>\n",
    "\n",
    "The [Human 3.6M dataset](http://vision.imar.ro/human3.6m/description.php) is a large-scale collection of 3.6 million 3D human poses captured from 11 professional actors in 17 everyday scenarios (e.g., talking, smoking, discussing). It includes synchronized video from multiple calibrated cameras, precise 3D joint positions and angles, and pixel-level body part labels. Additional data like time-of-flight range data and 3D laser scans of actors are also available.\n",
    "\n",
    "The dataset comes with precomputed image descriptors, tools for visualizing and predicting human poses, and an evaluation set for benchmarking performance, making it a rich resource for 3D human pose estimation, action recognition, and related computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76972dcfdd4a2b57",
   "metadata": {},
   "source": [
    "<h3><u>2.2. Dataset preparation </u></h3>\n",
    "\n",
    "Downloading the dataset from the website requires a login , therefore we will be directly using a preprocessed version of this dataset stored in google drive [here](https://drive.google.com/file/d/1JGt3j9q5A8WzUY-QKfyMVJUUvfreri-s/view?usp=drive_link).\n",
    " \n",
    "Original preprocessing was done by [Martinez et al](https://github.com/una-dinosauria/3d-pose-baseline) from this repository.\n",
    "\n",
    "The cells below go through the steps of downloading this preprocessed dataset and creating train-test files from it after some transformation. \n",
    "\n",
    "From section 3(Models) we will be directly using the created train-test files, therefore the cells under this section need not be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b437e917fa318b",
   "metadata": {},
   "source": [
    "#### Downloading the zip from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1e0ceefe13857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:31:40.978888Z",
     "start_time": "2024-10-24T10:31:06.728419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the zip from google drive\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# file id from gdrive (refer to markdown cell above for full link)\n",
    "file_id = '1JGt3j9q5A8WzUY-QKfyMVJUUvfreri-s' \n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Folder where you want to save the file\n",
    "folder_path = os.path.join(os.getcwd(),\"datasets\", \"h36m\", \"Original\")\n",
    "zip_file_path = os.path.join(folder_path, 'h36m.zip')\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Download the ZIP file\n",
    "gdown.download(download_url, zip_file_path, quiet=False)\n",
    "\n",
    "print(f\"Downloaded ZIP file and saved to {zip_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af82d9932cf7c2",
   "metadata": {},
   "source": [
    "#### Extracting the contents into the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0822847be0d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:31:43.844809Z",
     "start_time": "2024-10-24T10:31:40.978888Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting the contents inside the /datasets/h36m/Original folder\n",
    "import zipfile\n",
    "\n",
    "print(f\"Extracting files to {folder_path}..\")\n",
    "# Open the ZIP file and extract its contents\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(folder_path)\n",
    "\n",
    "print(f\"Finished extracting files to {folder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5da612c5f96",
   "metadata": {},
   "source": [
    "#### Saving the 3d positions into a compressed np file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e513820c9281fc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:31:50.599958Z",
     "start_time": "2024-10-24T10:31:43.845626Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "output_filename = os.path.join(folder_path, 'data_3d_h36m')\n",
    "subjects = ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11']\n",
    "\n",
    "output = dict()\n",
    "for subject in tqdm(subjects, desc= f'Processing subjects..'):\n",
    "    output[subject] = dict()\n",
    "    file_list = glob(os.path.join(folder_path, 'h36m', subject, 'MyPoses', '3D_positions', '*.h5'))\n",
    "    assert len(file_list) == 30, \"Expected 30 files for subject \" + subject + \", got \" + str(len(file_list))\n",
    "    for f in file_list:\n",
    "        action = os.path.splitext(os.path.basename(f))[0]\n",
    "\n",
    "        if subject == 'S11' and action == 'Directions':\n",
    "            continue  # Discard corrupted video\n",
    "\n",
    "        with h5py.File(f) as hf:\n",
    "            positions = hf.get('3D_positions')[:].reshape(32, 3, -1).transpose(2, 0, 1)\n",
    "            positions /= 1000  # Meters instead of millimeters\n",
    "            output[subject][action] = positions.astype('float32')\n",
    "\n",
    "print(f'Saving compressed 3d positions into {output_filename}')\n",
    "np.savez_compressed(output_filename, positions_3d=output)\n",
    "print(f'Finished saving 3d positions into {output_filename}')\n",
    "del output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588965a522e8f8",
   "metadata": {},
   "source": [
    "#### Compute the ground truth 2d poses.\n",
    "\n",
    "TODO.brandon explain what the human35mDataset constructor does in detail (intrinsics, extrinsics, cameras, normalization etc etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4794b93a1836eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:32:08.130629Z",
     "start_time": "2024-10-24T10:31:50.602485Z"
    }
   },
   "outputs": [],
   "source": [
    "from data.camera import world_to_camera, project_to_2d, image_coordinates, wrap\n",
    "from data.h36m_dataset import Human36mDataset\n",
    "\n",
    "print('Computing ground-truth 2D poses...')\n",
    "output_filename_2d = os.path.join(folder_path, 'data_2d_h36m_gt')\n",
    "\n",
    "dataset = Human36mDataset(output_filename + '.npz')\n",
    "output_2d_poses = {}\n",
    "for subject in dataset.subjects():\n",
    "    output_2d_poses[subject] = {}\n",
    "    for action in dataset[subject].keys():\n",
    "        anim = dataset[subject][action]\n",
    "\n",
    "        positions_2d = []\n",
    "        for cam in anim['cameras']:\n",
    "            pos_3d = world_to_camera(anim['positions'], R=cam['orientation'], t=cam['translation'])\n",
    "            pos_2d = wrap(project_to_2d, True, pos_3d, cam['intrinsic'])\n",
    "            pos_2d_pixel_space = image_coordinates(pos_2d, w=cam['res_w'], h=cam['res_h'])\n",
    "            positions_2d.append(pos_2d_pixel_space.astype('float32'))\n",
    "        output_2d_poses[subject][action] = positions_2d\n",
    "\n",
    "print(f'Saving compressed 2d positions into {output_filename_2d}')\n",
    "metadata = {\n",
    "    'num_joints': dataset.skeleton().num_joints(),\n",
    "    'keypoints_symmetry': [dataset.skeleton().joints_left(), dataset.skeleton().joints_right()]\n",
    "}\n",
    "np.savez_compressed(output_filename_2d, positions_2d=output_2d_poses, metadata=metadata)\n",
    "\n",
    "print(f'Done saving compressed 2d positions into {output_filename_2d}')\n",
    "del output_2d_poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72697b64fd7d407d",
   "metadata": {},
   "source": [
    "#### Cleanup the extracted files and the downloaded zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fd2a82b04cd62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:32:08.336197Z",
     "start_time": "2024-10-24T10:32:08.130629Z"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "\n",
    "rmtree(os.path.join(folder_path, 'h36m'))\n",
    "os.remove(os.path.join(folder_path, 'h36m.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ac66ee540da7d",
   "metadata": {},
   "source": [
    "#### Creating and saving train and test files from the saved npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb37ba45841bf03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:32:09.228186Z",
     "start_time": "2024-10-24T10:32:08.336197Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_utils import read_3d_data, create_2d_data, create_train_test_files\n",
    "from data.h36m_dataset import TRAIN_SUBJECTS, TEST_SUBJECTS, Human36mDataset\n",
    "import os\n",
    "\n",
    "subjects_train = TRAIN_SUBJECTS\n",
    "subjects_test = TEST_SUBJECTS\n",
    "\n",
    "processed_dataset_path = os.path.join(os.getcwd(), \"datasets\", \"h36m\", \"Processed\")\n",
    "os.makedirs(processed_dataset_path, exist_ok=True)\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"datasets\", \"h36m\", \"Original\", \"data_3d_h36m.npz\")\n",
    "dataset = Human36mDataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad8d6cc3eb740e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:32:12.228130Z",
     "start_time": "2024-10-24T10:32:09.228186Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO.brandon: add comments for what exactly the read_3d_data function does and the create_2d_data does.\n",
    "print(\"Reading 3d npz file\")\n",
    "dataset = read_3d_data(dataset)\n",
    "\n",
    "print(\"Reading 2d npz file\")\n",
    "dataset_2d_path = os.path.join(os.getcwd(), \"datasets\", \"h36m\", \"Original\", \"data_2d_h36m_gt.npz\")\n",
    "keypoints = create_2d_data(dataset_2d_path, dataset)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a656a402feba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:32:15.890233Z",
     "start_time": "2024-10-24T10:32:12.228130Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Creating train datasets and saving into file\")\n",
    "_, _, _ = create_train_test_files(subjects_train, dataset, keypoints, \"train\", processed_dataset_path)\n",
    "\n",
    "print(\"Creating test datasets and saving into file\")\n",
    "_, _, _ = create_train_test_files(subjects_test, dataset, keypoints, \"test\", processed_dataset_path)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c43c8d75e4e921",
   "metadata": {},
   "source": [
    "<h3><u>2.3 Visualizing poses </u></h3>\n",
    "\n",
    "Since we have saved the train and test files, we will now only use those files for the rest of the notebook.\n",
    "\n",
    "This has also been saved directly in this [google drive link](https://drive.google.com/drive/folders/1d38hWSM8clZlI11nqljxBECog1KY-iTd?usp=sharing), which can be placed directly in the local path instead of executing previous cells.\n",
    "\n",
    "The visualizations below shows that we are considering that human poses are comprised of skeletons with just 16 joints after doing all of the processing above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e7905edcd97d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:32:16.917256Z",
     "start_time": "2024-10-24T10:32:15.890233Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.visualization_utils import visualize_2d_pose, visualize_3d_pose\n",
    "\n",
    "poses_2d = np.load(\"datasets/h36m/Processed/test_2d_poses.npy\")\n",
    "poses_3d = np.load(\"datasets/h36m/Processed/test_3d_poses.npy\")\n",
    "\n",
    "visualize_2d_pose(poses_2d[0])\n",
    "visualize_3d_pose(poses_3d[0], elev=110, azim=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7725e8082f63bef6",
   "metadata": {},
   "source": [
    "#### Finding out the distribution of actions in the train and test splits\n",
    "\n",
    "The following code cells plots a bar graph of the actions in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4ee6d0caadd6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:42:37.509585Z",
     "start_time": "2024-10-24T10:42:36.546305Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_action_stats(actions, type):\n",
    "    train_actions_df = pd.DataFrame(actions, columns=['actions'])\n",
    "    action_counts = train_actions_df['actions'].value_counts()\n",
    "    print(action_counts)\n",
    "    \n",
    "    action_counts.plot(kind='bar')\n",
    "    \n",
    "    # Step 4: Show the plot\n",
    "    plt.title(f'{type} Action Distribution')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d339da3ad4be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:47:10.713481Z",
     "start_time": "2024-10-24T10:47:10.315303Z"
    }
   },
   "outputs": [],
   "source": [
    "train_actions = np.load(\"datasets/h36m/Processed/train_actions.npy\")\n",
    "show_action_stats(train_actions, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44918710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "w = nn.Parameter(torch.Tensor(size=(2,5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e77e8dbb1d9828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:47:11.880112Z",
     "start_time": "2024-10-24T10:47:11.560131Z"
    }
   },
   "outputs": [],
   "source": [
    "test_actions = np.load(\"datasets/h36m/Processed/test_actions.npy\")\n",
    "show_action_stats(train_actions, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19bb814deadd8b",
   "metadata": {},
   "source": [
    "<h2><u>3. Model Building Approach</u></h2>\n",
    "\n",
    "The following sections attempts to build different ML models using the dataset which was just constructed above.\n",
    "\n",
    "At a very high level these are the models we aim to build:\n",
    "* SimplePose - A simple baseline ML model using classic DL layers\n",
    "* SimplePoseGNN - A simple baseline GNN model using DGL and GNN layers\n",
    "* SemGCN - A reimplementation of the [SemGCN paper](https://arxiv.org/abs/1904.03345) in DGL\n",
    "* PoseGCN - A more complex GNN model by leveraging the insights gained from building the other GNN models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f319c5978c85c1",
   "metadata": {},
   "source": [
    "<h2><u>4. Baseline 1 - SimplePose (Simple ML model without using GNNs)</u></h2>\n",
    "\n",
    "TODO.brian - write something meaningful here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272c692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:34:12.664980Z",
     "start_time": "2024-10-24T11:34:08.681142Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.simple_pose.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 256, \n",
    "    'action_loss_multiplier': 0.005,\n",
    "    'pose_loss_multiplier': 0.995,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8d37443b92adc",
   "metadata": {},
   "source": [
    "<h2><u>5. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs)</u></h2>\n",
    "\n",
    "TODO.brian to write something meaningful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model is currently using : cuda\n",
      "INFO:root:Creating Missing Paths: model_outputs\\simple_pose_gnn\\2024-11-09--22-16-01\n",
      "INFO:root:Saving config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args are: Namespace(action_loss_multiplier=0.1, batch_size=256, learning_rate=0.0001, num_epochs=20, pose_loss_multiplier=0.9, save_path='model_outputs\\\\simple_pose_gnn\\\\2024-11-09--22-16-01', testing_2d_data_path='datasets\\\\h36m\\\\Processed\\\\test_2d_poses.npy', testing_3d_data_path='datasets\\\\h36m\\\\Processed\\\\test_3d_poses.npy', testing_label_path='datasets\\\\h36m\\\\Processed\\\\test_actions.npy', training_2d_data_path='datasets\\\\h36m\\\\Processed\\\\train_2d_poses.npy', training_3d_data_path='datasets\\\\h36m\\\\Processed\\\\train_3d_poses.npy', training_label_path='datasets\\\\h36m\\\\Processed\\\\train_actions.npy')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setup Training and Testing Dataloaders\n",
      "INFO:root:Setup SimplePoseGNN model\n",
      "INFO:root:SimplePoseGNN(\n",
      "  (input_layer): Linear(in_features=22, out_features=512, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x Sequential(\n",
      "      (0): GraphConvModule(\n",
      "        (conv_1): GraphConv(in=512, out=512, normalization=both, activation=None)\n",
      "        (batch_norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_2): GraphConv(in=512, out=512, normalization=both, activation=None)\n",
      "        (batch_norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.6, inplace=False)\n",
      "      )\n",
      "      (1): GraphConvModule(\n",
      "        (conv_1): GraphConv(in=512, out=512, normalization=both, activation=None)\n",
      "        (batch_norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_2): GraphConv(in=512, out=512, normalization=both, activation=None)\n",
      "        (batch_norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.6, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_3d_pose_linear): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.6, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): Dropout(p=0.6, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      "  (output_label_linear): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.6, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): Dropout(p=0.6, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=15, bias=True)\n",
      "  )\n",
      ")\n",
      "INFO:root:Setup Optimizer\n",
      "INFO:root:Setup loss functions\n",
      "INFO:root:Start Training and Testing Loops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EPOCH: 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training =>: 100%|██████████| 6093/6093 [16:24<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at epoch 0\n",
      "Epoch: 0 | Total Training Loss: 0.2124960571527481 | Pose Training Loss: 0.02483256533741951 | Action Training Loss: 1.9014689922332764 | Action Train Label Accuracy: 34.74167688196585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 2123/2123 [05:47<00:00,  6.12it/s]\n",
      "INFO:root:Saving model current state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Total Testing Loss: 0.2386551797389984 | Pose Testing Loss: 0.02460614964365959 | Action Testing Loss: 2.1650967597961426 | Action Test Label Accuracy: 28.280242351070406\n",
      "Starting EPOCH: 2 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training =>: 100%|██████████| 6093/6093 [17:16<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at epoch 1\n",
      "Epoch: 1 | Total Training Loss: 0.17890682816505432 | Pose Training Loss: 0.019155487418174744 | Action Training Loss: 1.61667001247406 | Action Train Label Accuracy: 44.34602423975094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 2123/2123 [05:28<00:00,  6.45it/s]\n",
      "INFO:root:Saving model current state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Total Testing Loss: 0.2471812516450882 | Pose Testing Loss: 0.02009672112762928 | Action Testing Loss: 2.290940284729004 | Action Test Label Accuracy: 27.744117906887716\n",
      "Starting EPOCH: 3 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training =>: 100%|██████████| 6093/6093 [20:41<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at epoch 2\n",
      "Epoch: 2 | Total Training Loss: 0.16328728199005127 | Pose Training Loss: 0.017220376059412956 | Action Training Loss: 1.4778857231140137 | Action Train Label Accuracy: 49.59237109489201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing =>: 100%|██████████| 2123/2123 [05:44<00:00,  6.16it/s]\n",
      "INFO:root:Saving model current state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Total Testing Loss: 0.27079156041145325 | Pose Testing Loss: 0.018641632050275803 | Action Testing Loss: 2.5401365756988525 | Action Test Label Accuracy: 26.94572867281133\n",
      "Starting EPOCH: 4 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training =>:   0%|          | 0/6093 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.simple_pose_gnn.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 0.005,\n",
    "    'pose_loss_multiplier': 0.995,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gnn', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386900b282f7968",
   "metadata": {},
   "source": [
    "<h2><u>6. Improvement 1 - SimplePoseGAT</u></h2>\n",
    "\n",
    "TODO.parash to write something meaningful here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf857d7fea859a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T07:03:41.225166Z",
     "start_time": "2024-11-02T07:03:21.503261Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.simple_pose_gat.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 256,\n",
    "    'action_loss_multiplier': 0.005,\n",
    "    'pose_loss_multiplier': 0.995,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'simple_pose_gat', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eddcac2a537037",
   "metadata": {},
   "source": [
    "<h2><u>7. Improvement 2 - SemGCN model (Reimplementation of Semantic GCN)</u></h2>\n",
    "\n",
    "TODO.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sem_gcn.train_and_test import training_loop\n",
    "from argparse import Namespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "args_dict = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 50,\n",
    "    'epoch_report': 2,\n",
    "    'batch_size': 256,\n",
    "    'training_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy'),\n",
    "    'training_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy'),\n",
    "    'training_label_path': os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy'),\n",
    "    'testing_2d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy'),\n",
    "    'testing_3d_data_path': os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy'),\n",
    "    'testing_label_path': os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy'),\n",
    "    'save_path': os.path.join('model_outputs', 'sem_gcn', timestamp)\n",
    "}\n",
    "args = Namespace(**args_dict)\n",
    "training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbad889b6bc016",
   "metadata": {},
   "source": [
    "<h2><u>8. Evaluation & Analysis of models<u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da5ad156e80a02",
   "metadata": {},
   "source": [
    "<h2><u>9. Creating our own custom dataset</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bbc24b9d697ad",
   "metadata": {},
   "source": [
    "<h2><u>10. Evaluation on custom dataset</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e11b1939fb52fe",
   "metadata": {},
   "source": [
    "<h2><u>11. Conclusion</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca78e7d8d35b85",
   "metadata": {},
   "source": [
    "<h2><u>12. Video presentation & Resources</u></h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
