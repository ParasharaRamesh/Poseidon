{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47024cb481badce7",
   "metadata": {},
   "source": [
    "# POSEIDON: Pose Estimation & Activity Recognition using GNNs\n",
    "\n",
    "Team Members (Group 16): \n",
    "1. Chong Jun Rong Brian (A0290882U)\n",
    "2. Parashara Ramesh (A0285647M)\n",
    "3. Ng Wei Jie Brandon (A0184893L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775158eb81e1a9",
   "metadata": {},
   "source": [
    "<h2><u> Table of contents </u></h2>\n",
    "\n",
    "1. What is this project about?\n",
    "<br> 1.1. Project Motivation\n",
    "<br> 1.2. Project Description\n",
    "<br> 1.3. Project Setup   \n",
    "2. Understanding the Human 3.6M Dataset\n",
    "3. Dataset preparation\n",
    "4. Models\n",
    "5. Baseline 1 - SimplePose (Simple ML model without using GNNs)\n",
    "6. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs) \n",
    "7. Improvement 1 - SemGCN model (Reimplementation of Semantic GCN)\n",
    "8. Improvement 2 - PoseGCN model (Tweaks of SemGCN)\n",
    "9. Evaluation & Analysis of models\n",
    "10. Creating our own custom dataset\n",
    "11. Evaluation on custom dataset\n",
    "12. Conclusion\n",
    "13. Video presentation & Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d0baed6e093c8",
   "metadata": {},
   "source": [
    "<h2><u>1. What is this project about?</u></h2>\n",
    "<h3><u>1.1 Project Motivation</u></h3>\n",
    "\n",
    "Accurately predicting 3D human poses from 2D keypoints is a critical task for many applications such as motion capture and activity recognition. Traditional methods that use direct regression or lifting techniques often struggle to fully capture the complex spatial relationships between body joints. By treating the 2D pose keypoints as graphs, we can leverage the underlying connectivity between joints to improve the 3D pose estimation. Additionally, recognizing and classifying human activities from these poses is an essential task in fields like surveillance and healthcare. Therefore, this project seeks to explore how GNNs can enhance 3D pose estimation and activity recognition.\n",
    "\n",
    "<h3><u>1.2 Project Description</h3></u>\n",
    "\n",
    "The primary objective of this project is to predict 3D human poses from 2D pose keypoints accurately using GNNs. \n",
    "* Firstly, we will develop two baseline models: one using standard Neural Network (NN) & Convolutional Neural Network (CNN) followed by a simple GNN based model both for 3d pose estimation \n",
    "* Secondly, we will reimplement the SemGCN model, which treats the body joints of a 2D pose as nodes in a graph, with edges representing the connectivity between them. \n",
    "* Finally, we will design an improved version of the SemGCN model by exploring different GNN architectures and modifications to enhance its performance.\n",
    "\n",
    "The secondary objective is to classify human activities based on 2D pose keypoints. We will use custom datasets to validate this task, allowing us to assess the generalization capabilities of GNN-based models for activity recognition.\n",
    "\n",
    "<h3><u>1.3 Project Setup</u></h3>\n",
    "\n",
    "1. Install the dependencies from requirements.txt (TODO.all to fix later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f092b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fd3e6fe88e214",
   "metadata": {},
   "source": [
    "<h2><u>2. Understanding the Human 3.6M Dataset</u></h2>\n",
    "\n",
    "TODO.brandon :\n",
    "- Add a small brief about the human 3.6M dataset + how we plan to use it\n",
    "- add h3 tags for each subsection + make changes in table of contents + only present the story points here, main code can go to appropriate folders\n",
    "\n",
    "EDA points will also come here as individual cell blocks but will be called as one function directly in the dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76972dcfdd4a2b57",
   "metadata": {},
   "source": [
    "<h2><u>3. Dataset preparation </u></h2>\n",
    "\n",
    "TODO.parash: create it once, and then mention that the code blocks under this section need not be run as they are present in this (drive/sharepoint folder)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19bb814deadd8b",
   "metadata": {},
   "source": [
    "<h2><u>4. Models</u></h2>\n",
    "\n",
    "TODO.all - write a convincing story on our approach + high level thoughts on why the following models are worth building and what we hope to gain from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0444a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Model based on A Simple yet effective baseline for 3D Pose Estimation\n",
    "class LinearBaselineModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_linear = nn.Linear(2, 1024) # 2D input shape is B x 16 x 2\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.output_linear = nn.Linear(1024, 3) # 3D output shape is B x 16 x 3\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = self.block1(x) + x # First Residual connection\n",
    "        x = self.block2(x) + x # Second Residual Connection\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f319c5978c85c1",
   "metadata": {},
   "source": [
    "<h2><u>5. Baseline 1 - SimplePose (Simple ML model without using GNNs)</u></h2>\n",
    "\n",
    "TODO.brian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091fa9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class Human36MDataset(Dataset):\n",
    "    def __init__(self, two_d_dataset_path, three_d_dataset_path, label_dataset_path):\n",
    "        self.two_d_dataset_path = two_d_dataset_path\n",
    "        self.three_d_dataset_path = three_d_dataset_path\n",
    "        self.label_dataset_path = label_dataset_path\n",
    "        self.input_data = np.load(self.two_d_dataset_path)\n",
    "        self.output_data = np.load(self.three_d_dataset_path)\n",
    "        self.labels = np.load(self.label_dataset_path)\n",
    "        unique_labels, tags = np.unique(self.labels, return_inverse=True)\n",
    "        self.labels = tags\n",
    "        self.labels_map = dict(zip(range(len(unique_labels)),unique_labels))\n",
    "        assert len(self.input_data) == len(self.labels) == len(self.output_data)\n",
    "    \n",
    "    def get_labels_map(self):\n",
    "        return self.labels_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return np.expand_dims(self.input_data[index], axis=0), np.expand_dims(self.output_data[index], axis=0), self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f272c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 4204555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24372/24372 [02:02<00:00, 198.51it/s]\n",
      "100%|██████████| 1/1 [02:03<00:00, 123.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8471620082855225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu') \n",
    "\n",
    "training_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy')\n",
    "training_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy')\n",
    "training_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy')\n",
    "training_data = Human36MDataset(training_2d_dataset_path, training_3d_dataset_path, training_label_path)\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "testing_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy')\n",
    "testing_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy')\n",
    "testing_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy')\n",
    "testing_data = Human36MDataset(testing_2d_dataset_path, testing_3d_dataset_path, testing_label_path)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "def kaiming_weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "# Declare Model\n",
    "model = LinearBaselineModel().to(DEVICE)\n",
    "# Apply Kaiming Init on Linear Layers\n",
    "model.apply(kaiming_weights_init)\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Declare Optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Declare Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96) # Value used by the original authors\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "global_losses = []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    local_losses = []\n",
    "    for data in tqdm(train_dataloader):\n",
    "        two_d_input_data, three_d_output_data, _= data\n",
    "        two_d_input_data = two_d_input_data.to(DEVICE)\n",
    "        three_d_output_data = three_d_output_data.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_3d_outputs = model(two_d_input_data)\n",
    "        loss = loss_fn(predicted_3d_outputs, three_d_output_data)\n",
    "        local_losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    current_loss = sum(local_losses) / len(train_dataloader)\n",
    "    global_losses.append(current_loss)\n",
    "    print(f\"Loss: {current_loss}\")\n",
    "    \n",
    "# Save model\n",
    "state_dict = {\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model': model.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "}\n",
    "weight_save_path = os.path.join('weights', 'linear_baseline_model')\n",
    "if not os.path.exists(weight_save_path):\n",
    "    os.makedirs(weight_save_path)\n",
    "\n",
    "torch.save(state_dict, os.path.join(weight_save_path, 'weights.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8d37443b92adc",
   "metadata": {},
   "source": [
    "<h2><u>6. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs)</u></h2>\n",
    "\n",
    "TODO.brandon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386900b282f7968",
   "metadata": {},
   "source": [
    "<h2><u>7. Improvement 1 - SemGCN model (Reimplementation of Semantic GCN)</u></h2>\n",
    "\n",
    "TODO.parash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eddcac2a537037",
   "metadata": {},
   "source": [
    "<h2><u>8. Improvement 2 - PoseGCN model (Tweaks of SemGCN)</u></h2>\n",
    "\n",
    "TODO.all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbad889b6bc016",
   "metadata": {},
   "source": [
    "<h2><u>9. Evaluation & Analysis of models<u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da5ad156e80a02",
   "metadata": {},
   "source": [
    "<h2><u>10. Creating our own custom dataset</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bbc24b9d697ad",
   "metadata": {},
   "source": [
    "<h2><u>11. Evaluation on custom dataset</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e11b1939fb52fe",
   "metadata": {},
   "source": [
    "<h2><u>12. Conclusion</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca78e7d8d35b85",
   "metadata": {},
   "source": [
    "<h2><u>13. Video presentation & Resources</u></h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
