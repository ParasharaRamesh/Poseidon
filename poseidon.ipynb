{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47024cb481badce7",
   "metadata": {},
   "source": [
    "# POSEIDON: Pose Estimation & Activity Recognition using GNNs\n",
    "\n",
    "Team Members (Group 16): \n",
    "1. Chong Jun Rong Brian (A0290882U)\n",
    "2. Parashara Ramesh (A0285647M)\n",
    "3. Ng Wei Jie Brandon (A0184893L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca470c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775158eb81e1a9",
   "metadata": {},
   "source": [
    "<h2><u> Table of contents </u></h2>\n",
    "\n",
    "1. What is this project about?\n",
    "<br> 1.1. Project Motivation\n",
    "<br> 1.2. Project Description\n",
    "<br> 1.3. Project Setup   \n",
    "2. Understanding the Human 3.6M Dataset\n",
    "3. Dataset preparation\n",
    "4. Models\n",
    "5. Baseline 1 - SimplePose (Simple ML model without using GNNs)\n",
    "6. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs) \n",
    "7. Improvement 1 - SemGCN model (Reimplementation of Semantic GCN)\n",
    "8. Improvement 2 - PoseGCN model (Tweaks of SemGCN)\n",
    "9. Evaluation & Analysis of models\n",
    "10. Creating our own custom dataset\n",
    "11. Evaluation on custom dataset\n",
    "12. Conclusion\n",
    "13. Video presentation & Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d0baed6e093c8",
   "metadata": {},
   "source": [
    "<h2><u>1. What is this project about?</u></h2>\n",
    "<h3><u>1.1 Project Motivation</u></h3>\n",
    "\n",
    "Accurately predicting 3D human poses from 2D keypoints is a critical task for many applications such as motion capture and activity recognition. Traditional methods that use direct regression or lifting techniques often struggle to fully capture the complex spatial relationships between body joints. By treating the 2D pose keypoints as graphs, we can leverage the underlying connectivity between joints to improve the 3D pose estimation. Additionally, recognizing and classifying human activities from these poses is an essential task in fields like surveillance and healthcare. Therefore, this project seeks to explore how GNNs can enhance 3D pose estimation and activity recognition.\n",
    "\n",
    "<h3><u>1.2 Project Description</h3></u>\n",
    "\n",
    "The primary objective of this project is to predict 3D human poses from 2D pose keypoints accurately using GNNs. \n",
    "* Firstly, we will develop two baseline models: one using standard Neural Network (NN) & Convolutional Neural Network (CNN) followed by a simple GNN based model both for 3d pose estimation \n",
    "* Secondly, we will reimplement the SemGCN model, which treats the body joints of a 2D pose as nodes in a graph, with edges representing the connectivity between them. \n",
    "* Finally, we will design an improved version of the SemGCN model by exploring different GNN architectures and modifications to enhance its performance.\n",
    "\n",
    "The secondary objective is to classify human activities based on 2D pose keypoints. We will use custom datasets to validate this task, allowing us to assess the generalization capabilities of GNN-based models for activity recognition.\n",
    "\n",
    "<h3><u>1.3 Project Setup</u></h3>\n",
    "\n",
    "1. Install the dependencies from requirements.txt (TODO.all to fix later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f092b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\" # DGL Settings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dgl.data import DGLDataset\n",
    "from utils.visualization_utils import visualize_2d_pose, visualize_3d_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fd3e6fe88e214",
   "metadata": {},
   "source": [
    "<h2><u>2. Understanding the Human 3.6M Dataset</u></h2>\n",
    "\n",
    "TODO.brandon :\n",
    "- Add a small brief about the human 3.6M dataset + how we plan to use it\n",
    "- add h3 tags for each subsection + make changes in table of contents + only present the story points here, main code can go to appropriate folders\n",
    "\n",
    "EDA points will also come here as individual cell blocks but will be called as one function directly in the dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4dced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_2d = np.load(\"datasets/h36m/Processed/test_2d_poses.npy\")\n",
    "poses_3d = np.load(\"datasets/h36m/Processed/test_3d_poses.npy\")\n",
    "visualize_2d_pose(poses_2d[0])\n",
    "visualize_3d_pose(poses_3d[0], elev=110, azim=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76972dcfdd4a2b57",
   "metadata": {},
   "source": [
    "<h2><u>3. Dataset preparation </u></h2>\n",
    "\n",
    "TODO.parash: create it once, and then mention that the code blocks under this section need not be run as they are present in this (drive/sharepoint folder)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19bb814deadd8b",
   "metadata": {},
   "source": [
    "<h2><u>4. Models</u></h2>\n",
    "\n",
    "TODO.all - write a convincing story on our approach + high level thoughts on why the following models are worth building and what we hope to gain from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0444a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Model based on A Simple yet effective baseline for 3D Pose Estimation\n",
    "class LinearBaselineModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, total_joints, total_actions):\n",
    "        super().__init__()\n",
    "        self.total_joints = total_joints,\n",
    "        self.total_actions = total_actions\n",
    "        self.input_linear = nn.Linear(total_joints * 2, 1024) # 1d input shape is B x 16 x 2\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.output_3d_pose_linear = nn.Linear(1024, total_joints * 3) # 3D output shape is B x 16 x 3\n",
    "        self.output_label_linear = nn.Linear(1024, total_actions) # Predict Action Labels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.input_linear(x)\n",
    "        x = self.block1(x) + x # First Residual connection\n",
    "        x = self.block2(x) + x # Second Residual Connection\n",
    "        three_dim_pose_predictions = self.output_3d_pose_linear(x)\n",
    "        action_label_predictions = self.output_label_linear(x)\n",
    "        joint_preds = three_dim_pose_predictions.view(x.shape[0], -1, 3)\n",
    "        action_preds = action_label_predictions\n",
    "        return joint_preds, action_preds   # 3D output shape is B x 16 x 3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f319c5978c85c1",
   "metadata": {},
   "source": [
    "<h2><u>5. Baseline 1 - SimplePose (Simple ML model without using GNNs)</u></h2>\n",
    "\n",
    "TODO.brian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "091fa9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class Human36MDataset(Dataset):\n",
    "    def __init__(self, two_dim_dataset_path, three_dim_dataset_path, label_dataset_path):\n",
    "        self.two_dim_dataset_path = two_dim_dataset_path\n",
    "        self.three_dim_dataset_path = three_dim_dataset_path\n",
    "        self.label_dataset_path = label_dataset_path\n",
    "        self.input_data = np.load(self.two_dim_dataset_path)\n",
    "        self.output_data = np.load(self.three_dim_dataset_path)\n",
    "        self.labels = np.load(self.label_dataset_path)\n",
    "        unique_labels, tags = np.unique(self.labels, return_inverse=True)\n",
    "        self.unique_labels = unique_labels\n",
    "        self.labels = tags\n",
    "        self.labels_map = dict(zip(range(len(unique_labels)),unique_labels))\n",
    "        assert len(self.input_data) == len(self.labels) == len(self.output_data)\n",
    "    \n",
    "    def get_action_numbers(self):\n",
    "        return len(self.unique_labels)\n",
    "    \n",
    "    def get_joint_numbers(self):\n",
    "        return self.input_data[0].shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_data[index], self.output_data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu') \n",
    "\n",
    "training_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy')\n",
    "training_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy')\n",
    "training_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy')\n",
    "training_data = Human36MDataset(training_2d_dataset_path, training_3d_dataset_path, training_label_path)\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testing_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'test_2d_poses.npy')\n",
    "testing_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'test_3d_poses.npy')\n",
    "testing_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'test_actions.npy')\n",
    "testing_data = Human36MDataset(testing_2d_dataset_path, testing_3d_dataset_path, testing_label_path)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "def kaiming_weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "TOTAL_JOINTS = training_data.get_joint_numbers()\n",
    "TOTAL_ACTIONS = training_data.get_action_numbers()\n",
    "\n",
    "# Declare Model\n",
    "model = LinearBaselineModel(TOTAL_JOINTS, TOTAL_ACTIONS).to(DEVICE)\n",
    "# Apply Kaiming Init on Linear Layers\n",
    "model.apply(kaiming_weights_init)\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Declare Optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Declare Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96) # Value used by the original authors\n",
    "\n",
    "# Loss Function\n",
    "three_dim_pose_estimation_loss_fn = nn.MSELoss()\n",
    "action_label_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "global_total_losses = []\n",
    "global_pose_losses = []\n",
    "global_action_losses = []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    predicted_labels = None\n",
    "    true_labels = None\n",
    "    total_losses = []\n",
    "    pose_losses = []\n",
    "    action_losses = []\n",
    "    for data in tqdm(train_dataloader):\n",
    "        # Prepare Data\n",
    "        two_dim_input_data, three_dim_output_data, action_labels= data\n",
    "        two_dim_input_data = two_dim_input_data.to(DEVICE)\n",
    "        three_dim_output_data = three_dim_output_data.to(DEVICE)\n",
    "        action_labels = action_labels.to(DEVICE)\n",
    "        # Set Gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        # Train Model\n",
    "        predicted_3d_pose_estimations, predicted_action_labels = model(two_dim_input_data)\n",
    "        # Calculate Loss\n",
    "        three_dim_pose_estimation_loss = three_dim_pose_estimation_loss_fn(predicted_3d_pose_estimations, three_dim_output_data)\n",
    "        action_label_loss = action_label_loss_fn(predicted_action_labels, action_labels)\n",
    "        loss = three_dim_pose_estimation_loss + action_label_loss\n",
    "        if epoch % 10 == 0: # Every 10 epochs, report once\n",
    "            # Store Results\n",
    "            total_losses.append(loss)\n",
    "            pose_losses.append(three_dim_pose_estimation_loss)\n",
    "            action_losses.append(action_label_loss)\n",
    "            predicted_action_labels = torch.argmax(predicted_action_labels, axis=1)\n",
    "            predicted_labels = predicted_action_labels if predicted_labels is None else torch.cat((predicted_labels, predicted_action_labels), axis=0)\n",
    "            true_labels = action_labels if true_labels is None else torch.cat((true_labels, action_labels), axis=0)\n",
    "        # Optimize Gradients and Update Learning Rate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Every 10 epochs, report once\n",
    "    if epoch % 10 == 0:\n",
    "        correct_predictions = (predicted_labels == true_labels).sum().item()\n",
    "        accuracy = correct_predictions / predicted_labels.size(0) * 100\n",
    "        total_loss = sum(total_losses) / len(train_dataloader)\n",
    "        pose_loss = sum(pose_losses) / len(train_dataloader)\n",
    "        action_loss = sum(action_losses) / len(train_dataloader)\n",
    "        global_total_losses.append(total_loss)\n",
    "        global_action_losses.append(action_loss)\n",
    "        global_pose_losses.append(pose_loss)\n",
    "        print(f\"Epoch: {epoch} | Total Training Loss: {total_loss} | Pose Training Loss: {pose_loss} | Action Training Loss: {action_loss} | Action Train Label Accuracy: {accuracy}\")\n",
    "    \n",
    "    if epoch % 10 == 0: # Every 10 epochs, test once\n",
    "        predicted_labels = None\n",
    "        true_labels = None\n",
    "        total_losses = []\n",
    "        pose_losses = []\n",
    "        action_losses = []\n",
    "        for data in tqdm(test_dataloader):\n",
    "            # Prepare Data\n",
    "            two_dim_input_data, three_dim_output_data, action_labels= data\n",
    "            two_dim_input_data = two_dim_input_data.to(DEVICE)\n",
    "            three_dim_output_data = three_dim_output_data.to(DEVICE)\n",
    "            action_labels = action_labels.to(DEVICE)\n",
    "            # Predict with model\n",
    "            predicted_3d_pose_estimations, predicted_action_labels = model(two_dim_input_data)\n",
    "            # Calculate Loss\n",
    "            three_dim_pose_estimation_loss = three_dim_pose_estimation_loss_fn(predicted_3d_pose_estimations, three_dim_output_data)\n",
    "            action_label_loss = action_label_loss_fn(predicted_action_labels, action_labels)\n",
    "            loss = three_dim_pose_estimation_loss + action_label_loss\n",
    "            # Store Results\n",
    "            total_losses.append(loss)\n",
    "            pose_losses.append(three_dim_pose_estimation_loss)\n",
    "            action_losses.append(action_label_loss)\n",
    "            predicted_action_labels = torch.argmax(predicted_action_labels, axis=1)\n",
    "            predicted_labels = predicted_action_labels if predicted_labels is None else torch.cat((predicted_labels, predicted_action_labels), axis=0)\n",
    "            true_labels = action_labels if true_labels is None else torch.cat((true_labels, action_labels), axis=0)\n",
    "        \n",
    "        # Calculate Test Accuracy\n",
    "        correct_predictions = (predicted_labels == true_labels).sum().item()\n",
    "        accuracy = correct_predictions / predicted_labels.size(0) * 100\n",
    "        total_loss = sum(total_losses) / len(train_dataloader)\n",
    "        pose_loss = sum(pose_losses) / len(train_dataloader)\n",
    "        action_loss = sum(action_losses) / len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch} | Total Testing Loss: {total_loss} | Pose Testing Loss: {pose_loss} | Action Testing Loss: {action_loss} | Action Test Label Accuracy: {accuracy}\")\n",
    "    \n",
    "# Save model\n",
    "state_dict = {\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model': model.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "}\n",
    "weight_save_path = os.path.join('weights', 'linear_baseline_model')\n",
    "if not os.path.exists(weight_save_path):\n",
    "    os.makedirs(weight_save_path)\n",
    "\n",
    "torch.save(state_dict, os.path.join(weight_save_path, 'weights.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8d37443b92adc",
   "metadata": {},
   "source": [
    "<h2><u>6. Baseline 2 - SimplePoseGNN (Simple ML model using GNNs)</u></h2>\n",
    "\n",
    "TODO.brandon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fc49055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=16, num_edges=16,\n",
       "      ndata_schemes={'feat': Scheme(shape=(2,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 2D Pose Dataset to 3D\n",
    "# Tutorial: https://docs.dgl.ai/tutorials/blitz/6_load_data.html\n",
    "# Source: https://arxiv.org/pdf/1904.03345 Appendix A\n",
    "\n",
    "class Human36MGraphDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"human_3.6m\")\n",
    "        \n",
    "    def process(self):\n",
    "        training_2d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_2d_poses.npy')\n",
    "        training_3d_dataset_path = os.path.join('datasets', 'h36m', 'Processed', 'train_3d_poses.npy')\n",
    "        training_label_path  = os.path.join('datasets', 'h36m', 'Processed', 'train_actions.npy')\n",
    "        # Datasets\n",
    "        two_dim_dataset = np.load(training_2d_dataset_path)\n",
    "        three_dim_dataset = np.load(training_3d_dataset_path)\n",
    "        label_dataset = np.load(training_label_path)\n",
    "        assert len(two_dim_dataset) == len(three_dim_dataset) == len(label_dataset)\n",
    "        # Edge Connections [Source & Destination] <-- Human Body Structure\n",
    "        human_pose_edge_src = torch.LongTensor([0, 0, 0, 1, 2, 4, 5, 7, 8, 8, 8, 10, 11, 8, 13, 14])\n",
    "        human_pose_edge_dst = torch.LongTensor([1, 4, 7, 2, 3, 5, 6, 8, 10, 13, 9, 11, 12, 13, 14, 15])\n",
    "        # # Edge Features\n",
    "        # for index in range(len(two_dim_dataset)):\n",
    "        #     two_dim_data, three_dim_data, label = two_dim_dataset[index], three_dim_dataset[index], label_dataset[index]\n",
    "        #     print(two_dim_data.shape, three_dim_data.shape, label.shape)\n",
    "            \n",
    "        \n",
    "        self.graph = dgl.graph(\n",
    "            (human_pose_edge_src, human_pose_edge_dst)\n",
    "        )\n",
    "        \n",
    "        self.graph.ndata[\"feat\"] = torch.Tensor(two_dim_dataset[0])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "graph = Human36MGraphDataset()\n",
    "graph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be089099",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://data.dgl.ai/tutorial/dataset/graph_edges.csv\", \"./graph_edges.csv\"\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://data.dgl.ai/tutorial/dataset/graph_properties.csv\",\n",
    "    \"./graph_properties.csv\",\n",
    ")\n",
    "edges = pd.read_csv(\"./graph_edges.csv\")\n",
    "properties = pd.read_csv(\"./graph_properties.csv\")\n",
    "\n",
    "edges.head()\n",
    "\n",
    "properties.head()\n",
    "\n",
    "\n",
    "class SyntheticDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"synthetic\")\n",
    "\n",
    "    def process(self):\n",
    "        edges = pd.read_csv(\"./graph_edges.csv\")\n",
    "        properties = pd.read_csv(\"./graph_properties.csv\")\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Create a graph for each graph ID from the edges table.\n",
    "        # First process the properties table into two dictionaries with graph IDs as keys.\n",
    "        # The label and number of nodes are values.\n",
    "        label_dict = {}\n",
    "        num_nodes_dict = {}\n",
    "        for _, row in properties.iterrows():\n",
    "            label_dict[row[\"graph_id\"]] = row[\"label\"]\n",
    "            num_nodes_dict[row[\"graph_id\"]] = row[\"num_nodes\"]\n",
    "\n",
    "        # For the edges, first group the table by graph IDs.\n",
    "        edges_group = edges.groupby(\"graph_id\")\n",
    "\n",
    "        # For each graph ID...\n",
    "        for graph_id in edges_group.groups:\n",
    "            # Find the edges as well as the number of nodes and its label.\n",
    "            edges_of_id = edges_group.get_group(graph_id)\n",
    "            src = edges_of_id[\"src\"].to_numpy()\n",
    "            dst = edges_of_id[\"dst\"].to_numpy()\n",
    "            num_nodes = num_nodes_dict[graph_id]\n",
    "            label = label_dict[graph_id]\n",
    "\n",
    "            # Create a graph and add it to the list of graphs and labels.\n",
    "            g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
    "            self.graphs.append(g)\n",
    "            self.labels.append(label)\n",
    "\n",
    "        # Convert the label list to tensor for saving.\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "dataset = SyntheticDataset()\n",
    "graph, label = dataset[0]\n",
    "print(graph, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386900b282f7968",
   "metadata": {},
   "source": [
    "<h2><u>7. Improvement 1 - SemGCN model (Reimplementation of Semantic GCN)</u></h2>\n",
    "\n",
    "TODO.parash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eddcac2a537037",
   "metadata": {},
   "source": [
    "<h2><u>8. Improvement 2 - PoseGCN model (Tweaks of SemGCN)</u></h2>\n",
    "\n",
    "TODO.all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbad889b6bc016",
   "metadata": {},
   "source": [
    "<h2><u>9. Evaluation & Analysis of models<u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da5ad156e80a02",
   "metadata": {},
   "source": [
    "<h2><u>10. Creating our own custom dataset</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bbc24b9d697ad",
   "metadata": {},
   "source": [
    "<h2><u>11. Evaluation on custom dataset</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e11b1939fb52fe",
   "metadata": {},
   "source": [
    "<h2><u>12. Conclusion</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca78e7d8d35b85",
   "metadata": {},
   "source": [
    "<h2><u>13. Video presentation & Resources</u></h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
